{
  "hash": "cc12e4c48d6cd9d7d8dac476d44832cc",
  "result": {
    "markdown": "---\ntitle: 4\\. Linear and Non-Linear Regression\nauthor: M Mubashar Ashraf\ndate: '2023-11-22'\ncategories:\n  - Linear Regression\n  - Non-Linear Regression\n  - ML\noutput: html_document\nimage: Reg.jpg\nformat:\n  html:\n    code-fold: true\ncode-fold: true\nkeep-ipynb: true\n---\n\n## **Introduction:**\n\nLinear and nonlinear regression are pillars of predictive modeling, allowing us to dissect and understand the relationships within datasets. In this exploration, we will embark on a journey through the intricacies of both linear and nonlinear regression. By understanding their nuances and applications, we aim to equip data scientists with the tools to decipher complex data relationships.\n\n### **Linear Regression: A Foundation for Interpretability:**\n\nLinear regression is a workhorse in the realm of predictive modeling, providing a clear and interpretable means of understanding relationships between variables. Let's delve into a sophisticated example:\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\n\n# Generate synthetic data for linear regression\nnp.random.seed(42)\nX_linear = np.random.rand(100, 1) * 10\ny_linear = 2 * X_linear + 1 + np.random.randn(100, 1) * 2\n\n# Fit a linear regression model\nmodel_linear = LinearRegression()\nmodel_linear.fit(X_linear, y_linear)\n\n# Predictions and evaluation\ny_pred_linear = model_linear.predict(X_linear)\nmse_linear = mean_squared_error(y_linear, y_pred_linear)\n\n# Create a DataFrame for better visualization\ndf_linear = pd.DataFrame({'Actual (Linear)': y_linear.flatten(), 'Predicted (Linear)': y_pred_linear.flatten()})\n\n# Visualize the linear regression line\nplt.scatter(X_linear, y_linear, alpha=0.8)\nplt.plot(X_linear, model_linear.predict(X_linear), color='red', linewidth=2)\nplt.title(f'Linear Regression (MSE: {mse_linear:.2f})')\nplt.xlabel('Independent Variable')\nplt.ylabel('Dependent Variable')\nplt.show()\n\n# Display the table for linear regression\nprint(\"Linear Regression Example:\")\nprint(df_linear.head(10))\nprint(\"\\nMean Squared Error (Linear):\", mse_linear)\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-2-output-1.png){width=585 height=449}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nLinear Regression Example:\n   Actual (Linear)  Predicted (Linear)\n0         8.664897            8.576588\n1        19.416271           19.570252\n2        15.823400           15.396969\n3         8.998032           12.852868\n4         3.681029            4.407099\n5         4.834116            4.406639\n6         5.117460            2.538454\n7        17.286982           17.957226\n8        11.405313           12.899739\n9        14.157937           14.940538\n\nMean Squared Error (Linear): 3.2263382558682134\n```\n:::\n:::\n\n\n*Explanation:* The code generates synthetic data, fits a linear regression model, evaluates predictions using mean squared error, and visualizes the data points along with the best-fit line. The table showcases the actual and predicted values for the first 10 observations.\n\n### **Mathematical Explanation of Linear Regression:**\n\n**Variables:**\n\n-   **Dependent Variable (Y):** The outcome we're predicting.\n\n-   **Independent Variable (X):** The variable used for predictions.\n\n**Equation:** Linear regression uses Y=mx+b.\n\n-   Y is the predicted value.\n\n-   m is the slope, indicating the change in Y for a one-unit change in X.\n\n-   x is the independent variable.\n\n-   b is the y-intercept, predicting Y when X is zero.\n\n**Objective:** Find the best-fitting line to represent the relationship between X and Y.\n\n**How It Works:**\n\n1.  Start with a random line.\n\n2.  Calculate predicted Y values.\n\n3.  Adjust the line to minimize differences between predicted and actual Y values.\n\n4.  Repeat until the line fits the data closely.\n\nHere's a simple example using Python and the scikit-learn library:\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\n\n# Create example data\nX = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)\nY = np.array([2, 4, 5, 4, 5])\n\n# Create and fit the linear regression model\nmodel = LinearRegression()\nmodel.fit(X, Y)\n\n# Predict Y values\nY_pred = model.predict(X)\n\n# Plot the data points and the regression line\nplt.scatter(X, Y, label='Actual Data')\nplt.plot(X, Y_pred, color='red', label='Regression Line')\nplt.xlabel('Independent Variable (X)')\nplt.ylabel('Dependent Variable (Y)')\nplt.legend()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-3-output-1.png){width=589 height=429}\n:::\n:::\n\n\nThis code generates a scatter plot with a regression line for a simple dataset. Adjust the values of X and Y for different data. The LinearRegression model is used to fit the data and make predictions. The resulting plot visualizes the relationship between the variables.\n\nThe last stage involves assessing the algorithm's performance, a crucial step to compare the effectiveness of various algorithms on a specific dataset. In the context of regression algorithms, three frequently utilized evaluation metrics come into play:\n\n**Mean Absolute Error (MAE):**\n\n-   **Formula:** MAE=n1​∑i=1n​∣Yactual,i​−Ypred,i​\n\n-   **Explanation:** It calculates the average absolute differences between the actual and predicted values.\n\n**Mean Squared Error (MSE):**\n\n-   **Formula:** MSE=n1​∑i=1n​(Yactual,i​−Ypred,i​)2\n\n-   **Explanation:** It calculates the average of the squared differences between actual and predicted values.\n\n**Root Mean Squared Error (RMSE):**\n\n-   **Formula:** RMSE=MSE​\n\n-   **Explanation:** It's the square root of MSE, providing a measure of the average magnitude of errors in the predictions.\n\nThese metrics help assess the performance of the linear regression model by quantifying the accuracy of predictions.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\n\n# Create example data\nX = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)\nY_actual = np.array([2, 4, 5, 4, 5])\n\n# Create and fit the linear regression model\nmodel = LinearRegression()\nmodel.fit(X, Y_actual)\n\n# Predict Y values\nY_pred = model.predict(X)\n\n# Calculate Mean Absolute Error (MAE)\nmae = mean_absolute_error(Y_actual, Y_pred)\n\n# Calculate Mean Squared Error (MSE)\nmse = mean_squared_error(Y_actual, Y_pred)\n\n# Calculate Root Mean Squared Error (RMSE)\nrmse = np.sqrt(mse)\n\n# Print evaluation metrics\nprint(f\"Mean Absolute Error (MAE): {mae}\")\nprint(f\"Mean Squared Error (MSE): {mse}\")\nprint(f\"Root Mean Squared Error (RMSE): {rmse}\")\n\n# Plot the data points and the regression line\nplt.scatter(X, Y_actual, label='Actual Data')\nplt.plot(X, Y_pred, color='red', label='Regression Line')\nplt.xlabel('Independent Variable (X)')\nplt.ylabel('Dependent Variable (Y)')\nplt.legend()\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMean Absolute Error (MAE): 0.6399999999999999\nMean Squared Error (MSE): 0.47999999999999987\nRoot Mean Squared Error (RMSE): 0.6928203230275508\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-2.png){width=589 height=429}\n:::\n:::\n\n\n### **Nonlinear Regression: Capturing Complexity with Polynomial Regression:**\n\nNonlinear regression steps in when relationships are more complex. Polynomial regression, a versatile technique, allows us to model curved patterns. Let's explore a more sophisticated example:\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.metrics import mean_squared_error\n\n# Generate synthetic data for nonlinear regression\nnp.random.seed(42)\nX_nonlinear = np.random.rand(100, 1) * 10\ny_nonlinear = 0.5 * X_nonlinear**2 - 2 * X_nonlinear + 1 + np.random.randn(100, 1) * 5\n\n# Fit a nonlinear regression model using polynomial features\ndegree_nonlinear = 2\nmodel_nonlinear = make_pipeline(PolynomialFeatures(degree_nonlinear), LinearRegression())\nmodel_nonlinear.fit(X_nonlinear, y_nonlinear)\n\n# Predictions and evaluation\ny_pred_nonlinear = model_nonlinear.predict(X_nonlinear)\nmse_nonlinear = mean_squared_error(y_nonlinear, y_pred_nonlinear)\n\n# Create a DataFrame for better visualization\ndf_nonlinear = pd.DataFrame({'Actual (Nonlinear)': y_nonlinear.flatten(), 'Predicted (Nonlinear)': y_pred_nonlinear.flatten()})\n\n# Visualize the nonlinear regression curve\nX_test_nonlinear = np.linspace(0, 10, 100).reshape(-1, 1)\nplt.scatter(X_nonlinear, y_nonlinear, alpha=0.8)\nplt.plot(X_test_nonlinear, model_nonlinear.predict(X_test_nonlinear), color='red', linewidth=2)\nplt.title(f'Nonlinear Regression (Polynomial Degree = {degree_nonlinear}, MSE: {mse_nonlinear:.2f})')\nplt.xlabel('Independent Variable')\nplt.ylabel('Dependent Variable')\nplt.show()\n\n# Display the table for nonlinear regression\nprint(\"\\nNonlinear Regression Example (Polynomial Degree = {}):\".format(degree_nonlinear))\nprint(df_nonlinear.head(10))\nprint(\"\\nMean Squared Error (Nonlinear):\", mse_nonlinear)\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-5-output-1.png){width=596 height=449}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\nNonlinear Regression Example (Polynomial Degree = 2):\n   Actual (Nonlinear)  Predicted (Nonlinear)\n0            0.958448              -0.137481\n1           25.683562              27.549222\n2           13.609682              12.221425\n3           -2.991415               5.769950\n4           -2.001641               0.063013\n5            0.882387               0.063360\n6            7.396483               2.062645\n7           18.598182              20.925834\n8            3.002195               5.868998\n9            9.398102              10.902700\n\nMean Squared Error (Nonlinear): 19.42984165875592\n```\n:::\n:::\n\n\n*Explanation:* The code generates synthetic data, fits a nonlinear regression model with polynomial features, evaluates predictions using mean squared error, and visualizes the data points along with the regression curve. The table showcases the actual and predicted values for the first 10 observations.\n\n### **Mathematical Explanation of Nonlinear Regression:**\n\nNonlinear regression is a statistical method used when the relationship between independent and dependent variables is not a straight line. It accommodates more complex patterns, allowing for curved relationships in the data.\n\n**Equation Form:** Unlike linear regression, the equation for nonlinear regression involves nonlinear functions and parameters. For instance, a possible form could be Y=(a⋅X^b)+c, where a, b, and c are parameters.\n\n**Objective:** Similar to linear regression, the aim is to find the best-fitting curve that represents the connection between variables. However, this curve can take various shapes based on the chosen nonlinear function.\n\n**Working Process:**\n\n1.  **Choose a Nonlinear Model:** Based on the data and understanding of the relationship, select a nonlinear function.\n\n2.  **Optimize Parameters:** Employ statistical methods to find the values of parameters that minimize the difference between predicted and actual values.\n\n3.  **Evaluate the Fit:** Assess the quality of fit using statistical metrics.\n\n**Evaluation Metrics:** Common metrics include Mean Absolute Error (MAE), Mean Squared Error (MSE), and Root Mean Squared Error (RMSE), similar to linear regression.\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\n\n# Create example data\nX = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)\nY_actual = np.array([2, 4, 5, 4, 5])\n\n# Transform features to include polynomial terms\npoly = PolynomialFeatures(degree=2)\nX_poly = poly.fit_transform(X)\n\n# Create and fit the nonlinear regression model\nmodel = LinearRegression()\nmodel.fit(X_poly, Y_actual)\n\n# Predict Y values\nY_pred = model.predict(X_poly)\n\n# Calculate evaluation metrics\nmae = mean_absolute_error(Y_actual, Y_pred)\nmse = mean_squared_error(Y_actual, Y_pred)\nrmse = np.sqrt(mse)\n\n# Print evaluation metrics\nprint(f\"Mean Absolute Error (MAE): {mae}\")\nprint(f\"Mean Squared Error (MSE): {mse}\")\nprint(f\"Root Mean Squared Error (RMSE): {rmse}\")\n\n# Plot the data points and the regression curve\nplt.scatter(X, Y_actual, label='Actual Data')\nplt.plot(X, Y_pred, color='red', label='Regression Curve')\nplt.xlabel('Independent Variable (X)')\nplt.ylabel('Dependent Variable (Y)')\nplt.legend()\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMean Absolute Error (MAE): 0.4457142857142868\nMean Squared Error (MSE): 0.2514285714285713\nRoot Mean Squared Error (RMSE): 0.5014265364224069\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-6-output-2.png){width=589 height=429}\n:::\n:::\n\n\nThis code demonstrates nonlinear regression using a polynomial function. Adjust the degree for different curve shapes. Evaluation metrics offer insights into model accuracy\n\n## **Conclusion:**\n\nLinear and nonlinear regression, when wielded with finesse, transform data into meaningful insights. In the sophisticated examples presented, we've not only explored their application but also enhanced the analysis with tables and plots. Whether unraveling linear relationships or capturing the intricacies of nonlinear patterns, these regression techniques are indispensable tools in the hands of data scientists. The journey into predictive modeling continues, with the nuanced understanding that the choice between linear and nonlinear regression depends on the underlying complexities of the dataset.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}