<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="M Mubashar Ashraf">
<meta name="dcterms.date" content="2023-11-21">

<title>Machine learning - 5. Classification</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../posts/6.0 Anomaly Outlier detection/index.html" rel="next">
<link href="../../posts/4.0 Linear and non-linear regression/index.html" rel="prev">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="../../styles.css">
<meta property="og:title" content="Machine learning - 5. Classification">
<meta property="og:description" content="">
<meta property="og:image" content="C.png">
<meta property="og:site-name" content="Machine learning">
<meta name="twitter:title" content="Machine learning - 5. Classification">
<meta name="twitter:description" content="">
<meta name="twitter:image" content="C.png">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-sidebar docked nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Machine learning</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link active" href="../../about.html" rel="" target="" aria-current="page">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/Mubashar97" rel="" target=""><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/Mubashar97/Intro-ML-Basics" rel="" target=""><i class="bi bi-house" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/mmubashar/" rel="" target=""><i class="bi bi-linkedin" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../posts/1.0 Machine Learning Introduction/index.html">Posts</a></li><li class="breadcrumb-item"><a href="../../posts/5.0 Classification/index.html">5. Classification</a></li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <div class="quarto-title-block"><div><h1 class="title">5. Classification</h1><button type="button" class="btn code-tools-button dropdown-toggle" id="quarto-code-tools-menu" data-bs-toggle="dropdown" aria-expanded="false"><i class="bi"></i> Code</button><ul class="dropdown-menu dropdown-menu-end" aria-labelelledby="quarto-code-tools-menu"><li><a id="quarto-show-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Show All Code</a></li><li><a id="quarto-hide-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Hide All Code</a></li><li><hr class="dropdown-divider"></li><li><a id="quarto-view-source" class="dropdown-item" href="javascript:void(0)" role="button">View Source</a></li></ul></div></div>
                                <div class="quarto-categories">
                <div class="quarto-category">Classification</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>M Mubashar Ashraf </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">November 21, 2023</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation docked overflow-auto">
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../about.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">About</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
 <span class="menu-text">Posts</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/1.0 Machine Learning Introduction/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">1. Machine Learning Introduction</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/2.0 Probability Theory and Random Variables/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">2. Probability Theory and Random Variables</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/3.0 Clustering/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">3. Clustering</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/4.0 Linear and non-linear regression/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">4. Linear and Non-Linear Regression</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/5.0 Classification/index.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">5. Classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/6.0 Anomaly Outlier detection/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">6. Anomaly Outlier Detection</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active" data-toc-expanded="2">
    <h2 id="toc-title">On this page</h2>
   
  <ul class="collapse">
  <li><a href="#binary-classification-discriminating-between-two-classes" id="toc-binary-classification-discriminating-between-two-classes" class="nav-link active" data-scroll-target="#binary-classification-discriminating-between-two-classes"><strong>Binary Classification: Discriminating Between Two Classes</strong></a></li>
  <li><a href="#multiclass-classification-navigating-diverse-class-labels" id="toc-multiclass-classification-navigating-diverse-class-labels" class="nav-link" data-scroll-target="#multiclass-classification-navigating-diverse-class-labels"><strong>Multiclass Classification: Navigating Diverse Class Labels</strong></a></li>
  <li><a href="#evaluation-metrics-and-beyond" id="toc-evaluation-metrics-and-beyond" class="nav-link" data-scroll-target="#evaluation-metrics-and-beyond"><strong>Evaluation Metrics and Beyond</strong></a></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion"><strong>Conclusion:</strong></a></li>
  </ul>
<div class="toc-actions"><div><i class="bi bi-github"></i></div><div class="action-links"><p><a href="https://github.com/Mubashar97/Intro-ML-Basics/blob/main/posts/5.0 Classification/index.qmd" class="toc-action">View source</a></p></div></div></nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<p><strong>Introduction:</strong> Classification, a fundamental task in machine learning, involves assigning predefined labels to instances based on their features. In this exploration, we delve into the intricacies of classification algorithms, showcasing their applications through Python examples. The journey will unravel the methodologies behind several classification techniques and demonstrate how to leverage them for insightful predictions.</p>
<section id="binary-classification-discriminating-between-two-classes" class="level2">
<h2 class="anchored" data-anchor-id="binary-classification-discriminating-between-two-classes"><strong>Binary Classification: Discriminating Between Two Classes</strong></h2>
<section id="logistic-regression" class="level3">
<h3 class="anchored" data-anchor-id="logistic-regression"><strong>Logistic Regression:</strong></h3>
<ul>
<li><p>Logistic regression is a type of statistical method used in machine learning for binary classification problems.</p></li>
<li><p>It’s a go-to algorithm when the outcome we want to predict falls into one of two categories, like whether an email is spam or not.</p></li>
<li><p>Unlike linear regression, which predicts continuous values, logistic regression predicts the probability that an instance belongs to a particular category.</p></li>
<li><p>The predicted probability is then transformed using a logistic function, squashing it between 0 and 1.</p></li>
<li><p>The outcome is classified based on a chosen threshold: if the probability is above the threshold, it’s one class; otherwise, it’s the other.</p></li>
</ul>
<p><strong>Key Features:</strong></p>
<ul>
<li><p><strong>Probabilistic Prediction:</strong> Logistic regression predicts the likelihood (probability) of an instance belonging to a specific class.</p></li>
<li><p><strong>Sigmoid Function:</strong> The logistic or sigmoid function is used to convert raw predictions into probabilities, ensuring they’re within the 0 to 1 range.</p></li>
<li><p><strong>Binary Outcome:</strong> Ideal for scenarios where the outcome is binary, like yes/no, spam/not spam, etc.</p></li>
<li><p><strong>Simple and Interpretable:</strong> Logistic regression is relatively simple, easy to understand, and provides interpretable results.</p></li>
</ul>
<p><strong>Applications:</strong></p>
<ul>
<li><p><strong>Spam Detection:</strong> Identifying whether an email is spam or not.</p></li>
<li><p><strong>Medical Diagnosis:</strong> Predicting if a patient has a particular medical condition based on test results.</p></li>
<li><p><strong>Credit Scoring:</strong> Assessing the likelihood of a customer defaulting on a loan.</p></li>
<li><p><strong>Customer Churn:</strong> Predicting whether a customer is likely to leave a service or not.</p></li>
</ul>
<p>Logistic Regression is a go-to algorithm for binary classification tasks.</p>
<div class="cell" data-execution_count="1">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_classification</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score, confusion_matrix</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate synthetic data for binary classification</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> make_classification(n_samples<span class="op">=</span><span class="dv">300</span>, n_features<span class="op">=</span><span class="dv">2</span>, n_informative<span class="op">=</span><span class="dv">2</span>, n_redundant<span class="op">=</span><span class="dv">0</span>, n_clusters_per_class<span class="op">=</span><span class="dv">1</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Split data into training and testing sets</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.25</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit a logistic regression model</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>model_logistic <span class="op">=</span> LogisticRegression()</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>model_logistic.fit(X_train, y_train)</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Predictions and evaluation</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>y_pred_logistic <span class="op">=</span> model_logistic.predict(X_test)</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>accuracy_logistic <span class="op">=</span> accuracy_score(y_test, y_pred_logistic)</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize decision boundary for logistic regression</span></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>plt.scatter(X[:, <span class="dv">0</span>], X[:, <span class="dv">1</span>], c<span class="op">=</span>y, cmap<span class="op">=</span>plt.cm.Paired, edgecolors<span class="op">=</span><span class="st">'k'</span>)</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Logistic Regression - Decision Boundary'</span>)</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Feature 1'</span>)</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Feature 2'</span>)</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Display confusion matrix for logistic regression</span></span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>conf_matrix_logistic <span class="op">=</span> confusion_matrix(y_test, y_pred_logistic)</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Logistic Regression Example:"</span>)</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Confusion Matrix:</span><span class="ch">\n</span><span class="st">"</span>, conf_matrix_logistic)</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Accuracy: </span><span class="sc">{</span>accuracy_logistic<span class="sc">:.2%}</span><span class="ss">"</span>)</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Explanation: Logistic Regression applied to binary classification with a visual representation of the decision boundary."</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-2-output-1.png" width="587" height="449"></p>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Logistic Regression Example:
Confusion Matrix:
 [[34  2]
 [ 1 38]]
Accuracy: 96.00%
Explanation: Logistic Regression applied to binary classification with a visual representation of the decision boundary.</code></pre>
</div>
</div>
</section>
<section id="support-vector-machines-svm" class="level3">
<h3 class="anchored" data-anchor-id="support-vector-machines-svm"><strong>Support Vector Machines (SVM):</strong></h3>
<ul>
<li><p>SVM is a versatile and powerful machine learning algorithm used for both classification and regression tasks.</p></li>
<li><p>Its primary goal is to find the optimal hyperplane that best separates different classes in the data.</p></li>
<li><p>SVM is especially effective in high-dimensional spaces and is robust even when the number of features exceeds the number of samples.</p></li>
<li><p>It’s a supervised learning algorithm, meaning it learns from labeled training data to make predictions on unseen data.</p></li>
</ul>
<p><strong>Key Features:</strong></p>
<ul>
<li><p><strong>Maximizing Margin:</strong> SVM aims to find a hyperplane with the maximum margin, the distance between the hyperplane and the nearest data point from each class.</p></li>
<li><p><strong>Support Vectors:</strong> The critical data points that determine the position of the optimal hyperplane are called support vectors.</p></li>
<li><p><strong>Kernel Trick:</strong> SVM can handle non-linear relationships between features by using a kernel function to map the data into a higher-dimensional space.</p></li>
<li><p><strong>Versatility:</strong> SVM can be used for binary and multiclass classification as well as regression tasks.</p></li>
</ul>
<p><strong>Applications:</strong></p>
<ul>
<li><p><strong>Image Classification:</strong> SVM is used in image recognition tasks to classify objects in images.</p></li>
<li><p><strong>Text Classification:</strong> It’s employed in natural language processing for tasks like spam detection and sentiment analysis.</p></li>
<li><p><strong>Bioinformatics:</strong> SVM helps in classifying biological data, such as predicting protein functions.</p></li>
<li><p><strong>Finance:</strong> SVM is used in predicting stock prices and credit scoring.</p></li>
</ul>
<p>Support Vector Machines are valued for their ability to handle complex relationships in data and find robust decision boundaries, making them applicable to various real-world problems.</p>
<p>Support Vector Machines offer robust performance in binary classification scenarios.</p>
<div class="cell" data-execution_count="2">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> generate_random_dataset(size):</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>    <span class="co">""" Generate a random dataset and that follows a quadratic  distribution</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> []</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> []</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>    target <span class="op">=</span> []</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(size):</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>        <span class="co"># class zero</span></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>        x.append(np.<span class="bu">round</span>(random.uniform(<span class="dv">0</span>, <span class="fl">2.5</span>), <span class="dv">1</span>))</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>        y.append(np.<span class="bu">round</span>(random.uniform(<span class="dv">0</span>, <span class="dv">20</span>), <span class="dv">1</span>))</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>        target.append(<span class="dv">0</span>)</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>        <span class="co"># class one</span></span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>        x.append(np.<span class="bu">round</span>(random.uniform(<span class="dv">1</span>, <span class="dv">5</span>), <span class="dv">2</span>))</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>        y.append(np.<span class="bu">round</span>(random.uniform(<span class="dv">20</span>, <span class="dv">25</span>), <span class="dv">2</span>))</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>        target.append(<span class="dv">1</span>)</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>        x.append(np.<span class="bu">round</span>(random.uniform(<span class="dv">3</span>, <span class="dv">5</span>), <span class="dv">2</span>))</span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>        y.append(np.<span class="bu">round</span>(random.uniform(<span class="dv">5</span>, <span class="dv">25</span>), <span class="dv">2</span>))</span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>        target.append(<span class="dv">1</span>)</span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a>    df_x <span class="op">=</span> pd.DataFrame(data<span class="op">=</span>x)</span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a>    df_y <span class="op">=</span> pd.DataFrame(data<span class="op">=</span>y)</span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a>    df_target <span class="op">=</span> pd.DataFrame(data<span class="op">=</span>target)</span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a>    data_frame <span class="op">=</span> pd.concat([df_x, df_y], ignore_index<span class="op">=</span><span class="va">True</span>, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a>    data_frame <span class="op">=</span> pd.concat([data_frame, df_target], ignore_index<span class="op">=</span><span class="va">True</span>, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a>    data_frame.columns <span class="op">=</span> [<span class="st">'x'</span>, <span class="st">'y'</span>, <span class="st">'target'</span>]</span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> data_frame</span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-31"><a href="#cb3-31" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate dataset</span></span>
<span id="cb3-32"><a href="#cb3-32" aria-hidden="true" tabindex="-1"></a>size <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb3-33"><a href="#cb3-33" aria-hidden="true" tabindex="-1"></a>dataset <span class="op">=</span> generate_random_dataset(size)</span>
<span id="cb3-34"><a href="#cb3-34" aria-hidden="true" tabindex="-1"></a>features <span class="op">=</span> dataset[[<span class="st">'x'</span>, <span class="st">'y'</span>]]</span>
<span id="cb3-35"><a href="#cb3-35" aria-hidden="true" tabindex="-1"></a>label <span class="op">=</span> dataset[<span class="st">'target'</span>]</span>
<span id="cb3-36"><a href="#cb3-36" aria-hidden="true" tabindex="-1"></a><span class="co"># Hold out 20% of the dataset for training</span></span>
<span id="cb3-37"><a href="#cb3-37" aria-hidden="true" tabindex="-1"></a>test_size <span class="op">=</span> <span class="bu">int</span>(np.<span class="bu">round</span>(size <span class="op">*</span> <span class="fl">0.2</span>, <span class="dv">0</span>))</span>
<span id="cb3-38"><a href="#cb3-38" aria-hidden="true" tabindex="-1"></a><span class="co"># Split dataset into training and testing sets</span></span>
<span id="cb3-39"><a href="#cb3-39" aria-hidden="true" tabindex="-1"></a>x_train <span class="op">=</span> features[:<span class="op">-</span>test_size].values</span>
<span id="cb3-40"><a href="#cb3-40" aria-hidden="true" tabindex="-1"></a>y_train <span class="op">=</span> label[:<span class="op">-</span>test_size].values</span>
<span id="cb3-41"><a href="#cb3-41" aria-hidden="true" tabindex="-1"></a>x_test <span class="op">=</span> features[<span class="op">-</span>test_size:].values</span>
<span id="cb3-42"><a href="#cb3-42" aria-hidden="true" tabindex="-1"></a>y_test <span class="op">=</span> label[<span class="op">-</span>test_size:].values</span>
<span id="cb3-43"><a href="#cb3-43" aria-hidden="true" tabindex="-1"></a><span class="co"># Plotting the training set</span></span>
<span id="cb3-44"><a href="#cb3-44" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">7</span>))</span>
<span id="cb3-45"><a href="#cb3-45" aria-hidden="true" tabindex="-1"></a><span class="co"># removing to and right border</span></span>
<span id="cb3-46"><a href="#cb3-46" aria-hidden="true" tabindex="-1"></a>ax.spines[<span class="st">'top'</span>].set_visible(<span class="va">False</span>)</span>
<span id="cb3-47"><a href="#cb3-47" aria-hidden="true" tabindex="-1"></a>ax.spines[<span class="st">'left'</span>].set_visible(<span class="va">False</span>)</span>
<span id="cb3-48"><a href="#cb3-48" aria-hidden="true" tabindex="-1"></a>ax.spines[<span class="st">'right'</span>].set_visible(<span class="va">False</span>)</span>
<span id="cb3-49"><a href="#cb3-49" aria-hidden="true" tabindex="-1"></a><span class="co"># adding major gridlines</span></span>
<span id="cb3-50"><a href="#cb3-50" aria-hidden="true" tabindex="-1"></a>ax.grid(color<span class="op">=</span><span class="st">'grey'</span>, linestyle<span class="op">=</span><span class="st">'-'</span>, linewidth<span class="op">=</span><span class="fl">0.25</span>, alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb3-51"><a href="#cb3-51" aria-hidden="true" tabindex="-1"></a>ax.scatter(features[:<span class="op">-</span>test_size][<span class="st">'x'</span>], features[:<span class="op">-</span>test_size][<span class="st">'y'</span>], color<span class="op">=</span><span class="st">"#8C7298"</span>)</span>
<span id="cb3-52"><a href="#cb3-52" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-3-output-1.png" width="938" height="559"></p>
</div>
</div>
</section>
</section>
<section id="multiclass-classification-navigating-diverse-class-labels" class="level2">
<h2 class="anchored" data-anchor-id="multiclass-classification-navigating-diverse-class-labels"><strong>Multiclass Classification: Navigating Diverse Class Labels</strong></h2>
<section id="random-forest" class="level3">
<h3 class="anchored" data-anchor-id="random-forest"><strong>Random Forest:</strong></h3>
<ul>
<li><p>Random Forest is a powerful ensemble learning algorithm used for both classification and regression tasks.</p></li>
<li><p>It operates by constructing multiple decision trees during training and outputs the mode of the classes (classification) or the mean prediction (regression) of the individual trees.</p></li>
<li><p>It’s an ensemble method, meaning it combines the predictions of multiple models to improve overall accuracy and robustness.</p></li>
<li><p>The “forest” in Random Forest is a collection of decision trees, and the “random” part comes from the introduction of randomness during both training and prediction phases.</p></li>
</ul>
<p><strong>Key Features:</strong></p>
<ul>
<li><p><strong>Decision Trees:</strong> Random Forest is built on the foundation of decision trees, which are known for their simplicity and interpretability.</p></li>
<li><p><strong>Bootstrap Aggregating (Bagging):</strong> Random Forest employs bagging, a technique that involves training each tree on a random subset of the data, allowing for diverse trees.</p></li>
<li><p><strong>Feature Randomness:</strong> During the construction of each tree, a random subset of features is considered at each split, enhancing the model’s diversity.</p></li>
<li><p><strong>Reducing Overfitting:</strong> The combination of diverse trees and feature randomness helps mitigate overfitting, making Random Forest robust to noisy data.</p></li>
</ul>
<p><strong>Applications:</strong></p>
<ul>
<li><p><strong>Image Classification:</strong> Random Forest is used in image recognition tasks to classify objects in images.</p></li>
<li><p><strong>Healthcare:</strong> It’s applied in predicting diseases based on patient data and medical images.</p></li>
<li><p><strong>Finance:</strong> Random Forest is employed in credit scoring and fraud detection.</p></li>
<li><p><strong>Ecology:</strong> In ecology, it helps analyze and predict biodiversity patterns.</p></li>
</ul>
<p>Random Forest is valued for its ability to handle complex relationships in data, provide robust predictions, and mitigate overfitting, making it a widely used algorithm in various domains.</p>
<p>Random Forest is an ensemble method known for its versatility in handling multiclass classification.</p>
<div class="cell" data-execution_count="3">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> load_breast_cancer</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn <span class="im">import</span> tree</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> RandomForestClassifier</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the Breast Cancer Dataset</span></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> load_breast_cancer()</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.DataFrame(data.data, columns<span class="op">=</span>data.feature_names)</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>df[<span class="st">'target'</span>] <span class="op">=</span> data.target</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Arrange Data into Features Matrix and Target Vector</span></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> df.loc[:, df.columns <span class="op">!=</span> <span class="st">'target'</span>]</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> df.loc[:, <span class="st">'target'</span>].values</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Split the data into training and testing sets</span></span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>X_train, X_test, Y_train, Y_test <span class="op">=</span> train_test_split(X, y, random_state<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Random Forests in `scikit-learn` (with N = 100)</span></span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>rf <span class="op">=</span> RandomForestClassifier(n_estimators<span class="op">=</span><span class="dv">100</span>,</span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>                            random_state<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a>rf.fit(X_train, Y_train)</span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a>fn<span class="op">=</span>data.feature_names</span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a>cn<span class="op">=</span>data.target_names</span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(nrows <span class="op">=</span> <span class="dv">1</span>,ncols <span class="op">=</span> <span class="dv">1</span>,figsize <span class="op">=</span> (<span class="dv">4</span>,<span class="dv">4</span>), dpi<span class="op">=</span><span class="dv">800</span>)</span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a>tree.plot_tree(rf.estimators_[<span class="dv">0</span>],</span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a>               feature_names <span class="op">=</span> fn, </span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a>               class_names<span class="op">=</span>cn,</span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a>               filled <span class="op">=</span> <span class="va">True</span>)<span class="op">;</span></span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a>fig.savefig(<span class="st">'rf_individualtree.png'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-4-output-1.png" class="img-fluid"></p>
</div>
</div>
</section>
</section>
<section id="evaluation-metrics-and-beyond" class="level2">
<h2 class="anchored" data-anchor-id="evaluation-metrics-and-beyond"><strong>Evaluation Metrics and Beyond</strong></h2>
<section id="receiver-operating-characteristic-roc-curve" class="level3">
<h3 class="anchored" data-anchor-id="receiver-operating-characteristic-roc-curve"><strong>Receiver Operating Characteristic (ROC) Curve:</strong></h3>
<p>ROC curves visualize the trade-off between true positive rate and false positive rate.</p>
<div class="cell" data-execution_count="4">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> load_breast_cancer</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.svm <span class="im">import</span> SVC</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> roc_curve, auc</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the Breast Cancer dataset</span></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>cancer <span class="op">=</span> load_breast_cancer()</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> cancer.data</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> cancer.target</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert the problem to binary classification (class 0 vs class 1)</span></span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>y_binary <span class="op">=</span> (y <span class="op">==</span> <span class="dv">0</span>).astype(<span class="bu">int</span>)</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Split the data into training and testing sets</span></span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y_binary, test_size<span class="op">=</span><span class="fl">0.25</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a simple SVM model for binary classification</span></span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>svm_model <span class="op">=</span> SVC(kernel<span class="op">=</span><span class="st">'linear'</span>, probability<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>svm_model.fit(X_train, y_train)</span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Obtain class probabilities for positive class (class 0)</span></span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a>y_probabilities <span class="op">=</span> svm_model.predict_proba(X_test)[:, <span class="dv">1</span>]</span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute ROC curve and AUC</span></span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a>fpr, tpr, _ <span class="op">=</span> roc_curve(y_test, y_probabilities)</span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a>roc_auc <span class="op">=</span> auc(fpr, tpr)</span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the ROC curve</span></span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">6</span>))</span>
<span id="cb5-32"><a href="#cb5-32" aria-hidden="true" tabindex="-1"></a>plt.plot(fpr, tpr, color<span class="op">=</span><span class="st">'darkorange'</span>, lw<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="ss">f'ROC curve (AUC = </span><span class="sc">{</span>roc_auc<span class="sc">:.2f}</span><span class="ss">)'</span>)</span>
<span id="cb5-33"><a href="#cb5-33" aria-hidden="true" tabindex="-1"></a>plt.plot([<span class="dv">0</span>, <span class="dv">1</span>], [<span class="dv">0</span>, <span class="dv">1</span>], color<span class="op">=</span><span class="st">'navy'</span>, lw<span class="op">=</span><span class="dv">2</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, label<span class="op">=</span><span class="st">'Random'</span>)</span>
<span id="cb5-34"><a href="#cb5-34" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'False Positive Rate'</span>)</span>
<span id="cb5-35"><a href="#cb5-35" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'True Positive Rate'</span>)</span>
<span id="cb5-36"><a href="#cb5-36" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Receiver Operating Characteristic (ROC) Curve'</span>)</span>
<span id="cb5-37"><a href="#cb5-37" aria-hidden="true" tabindex="-1"></a>plt.legend(loc<span class="op">=</span><span class="st">'lower right'</span>)</span>
<span id="cb5-38"><a href="#cb5-38" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-5-output-1.png" width="663" height="523"></p>
</div>
</div>
<p>This code uses the Breast Cancer dataset, and you can replace it with any binary classification dataset of your choice. The ROC curve is a useful tool for evaluating the performance of binary classification models.</p>
</section>
<section id="precision-recall-curve" class="level3">
<h3 class="anchored" data-anchor-id="precision-recall-curve"><strong>Precision-Recall Curve:</strong></h3>
<p>Precision-Recall curves help evaluate models under different class distribution scenarios.</p>
<div class="cell" data-execution_count="5">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> load_breast_cancer</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.svm <span class="im">import</span> SVC</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> precision_recall_curve, auc, average_precision_score</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the Breast Cancer dataset</span></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>cancer <span class="op">=</span> load_breast_cancer()</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> cancer.data</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> cancer.target</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert the problem to binary classification (class 0 vs class 1)</span></span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>y_binary <span class="op">=</span> (y <span class="op">==</span> <span class="dv">0</span>).astype(<span class="bu">int</span>)</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Split the data into training and testing sets</span></span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y_binary, test_size<span class="op">=</span><span class="fl">0.25</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a simple SVM model for binary classification</span></span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a>svm_model <span class="op">=</span> SVC(kernel<span class="op">=</span><span class="st">'linear'</span>, probability<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a>svm_model.fit(X_train, y_train)</span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Obtain class probabilities for positive class (class 0)</span></span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a>y_probabilities <span class="op">=</span> svm_model.predict_proba(X_test)[:, <span class="dv">1</span>]</span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-26"><a href="#cb6-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute Precision-Recall curve and AUC</span></span>
<span id="cb6-27"><a href="#cb6-27" aria-hidden="true" tabindex="-1"></a>precision, recall, _ <span class="op">=</span> precision_recall_curve(y_test, y_probabilities)</span>
<span id="cb6-28"><a href="#cb6-28" aria-hidden="true" tabindex="-1"></a>pr_auc <span class="op">=</span> auc(recall, precision)</span>
<span id="cb6-29"><a href="#cb6-29" aria-hidden="true" tabindex="-1"></a>average_precision <span class="op">=</span> average_precision_score(y_test, y_probabilities)</span>
<span id="cb6-30"><a href="#cb6-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-31"><a href="#cb6-31" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the Precision-Recall curve</span></span>
<span id="cb6-32"><a href="#cb6-32" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">6</span>))</span>
<span id="cb6-33"><a href="#cb6-33" aria-hidden="true" tabindex="-1"></a>plt.plot(recall, precision, color<span class="op">=</span><span class="st">'darkorange'</span>, lw<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="ss">f'PR curve (AUC = </span><span class="sc">{</span>pr_auc<span class="sc">:.2f}</span><span class="ss">, Avg. Precision = </span><span class="sc">{</span>average_precision<span class="sc">:.2f}</span><span class="ss">)'</span>)</span>
<span id="cb6-34"><a href="#cb6-34" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Recall'</span>)</span>
<span id="cb6-35"><a href="#cb6-35" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Precision'</span>)</span>
<span id="cb6-36"><a href="#cb6-36" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Precision-Recall Curve'</span>)</span>
<span id="cb6-37"><a href="#cb6-37" aria-hidden="true" tabindex="-1"></a>plt.legend(loc<span class="op">=</span><span class="st">'lower right'</span>)</span>
<span id="cb6-38"><a href="#cb6-38" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-6-output-1.png" width="663" height="523"></p>
</div>
</div>
<p>This code demonstrates how to create a Precision-Recall curve for a binary classification problem using an SVM model. Replace the dataset with any binary classification dataset of your choice. The Precision-Recall curve provides valuable insights into the performance of a classifier, especially in imbalanced datasets.</p>
</section>
<section id="mathematical-explanation-of-classification-in-machine-learning" class="level3">
<h3 class="anchored" data-anchor-id="mathematical-explanation-of-classification-in-machine-learning"><strong>Mathematical Explanation of Classification in Machine Learning:</strong></h3>
<p>Classification in machine learning is a type of supervised learning where the algorithm learns to categorize input data into predefined classes or labels. The goal is to train the model to make accurate predictions on new, unseen data based on patterns learned during training.</p>
<p><strong>Binary Classification:</strong> In binary classification, there are two possible outcomes or classes, often denoted as positive (1) and negative (0). The algorithm learns a decision boundary to separate these classes in the feature space.</p>
<p>Here are the mathematical formulas for key binary classification metrics:</p>
<p><strong>True Positive (TP):</strong></p>
<p>TP=Number&nbsp;of&nbsp;instances&nbsp;correctly&nbsp;predicted&nbsp;as&nbsp;positive</p>
<p><strong>True Negative (TN):</strong></p>
<p>TN=Number&nbsp;of&nbsp;instances&nbsp;correctly&nbsp;predicted&nbsp;as&nbsp;negative</p>
<p><strong>False Positive (FP):</strong></p>
<p>FP=Number&nbsp;of&nbsp;instances&nbsp;incorrectly&nbsp;predicted&nbsp;as&nbsp;positive&nbsp;(Type&nbsp;I&nbsp;error)</p>
<p><strong>False Negative (FN):</strong></p>
<p>FN=Number&nbsp;of&nbsp;instances&nbsp;incorrectly&nbsp;predicted&nbsp;as&nbsp;negative&nbsp;(Type&nbsp;II&nbsp;error)</p>
<p><strong>Accuracy:</strong></p>
<p>Accuracy=TP+TN/(FP+FN+TP+TN)​</p>
<p><strong>Precision (Positive Predictive Value):</strong></p>
<p>Precision=TP/(FP+TP)​</p>
<p><strong>Recall (Sensitivity, True Positive Rate):</strong></p>
<p>Recall=TP/(FN+TP)​</p>
<p><strong>Specificity (True Negative Rate):</strong></p>
<p>Specificity=TN/(FP+TN)​</p>
<p><strong>F1 Score:</strong></p>
<p>F1Score=(2⋅Precision⋅Recall​)/(Precision+Recall)</p>
<p><strong>False Positive Rate (FPR):</strong></p>
<p>FPR=FP/(TN+FP)​</p>
<p><strong>False Negative Rate (FNR):</strong></p>
<p>FNR=FN/(TP+FN)​</p>
<p><strong>Area Under the ROC Curve (AUC-ROC):</strong></p>
<p>AUC-ROC(Area&nbsp;Under&nbsp;the&nbsp;Receiver&nbsp;Operating&nbsp;Characteristic&nbsp;Curve)</p>
<p>These formulas provide a quantitative assessment of a binary classification model’s performance by considering different aspects such as accuracy, precision, recall, specificity, and the trade-off between precision and recall captured by the F1 score. The AUC-ROC measures the area under the curve that illustrates the model’s true positive rate against the false positive rate at various decision thresholds.</p>
<p><strong>Multiclass Classification:</strong> For multiclass classification, there are more than two classes, and the model learns to distinguish between all possible classes. It involves assigning an input to one of several predefined categories.</p>
<p><strong>Logistic Regression:</strong> Logistic Regression is a commonly used algorithm for binary classification. It models the probability of an instance belonging to a particular class using the logistic function. The decision boundary is determined by a set of weights and biases.</p>
<p><strong>Support Vector Machines (SVM):</strong> SVM is another algorithm that finds the optimal hyperplane to separate classes. It works well for both binary and multiclass classification, aiming to maximize the margin between different classes.</p>
<div class="cell" data-execution_count="6">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> roc_curve, auc, precision_recall_curve, confusion_matrix</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Create example data</span></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.random.rand(<span class="dv">100</span>, <span class="dv">2</span>)</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> (X[:, <span class="dv">0</span>] <span class="op">+</span> X[:, <span class="dv">1</span>] <span class="op">&gt;</span> <span class="dv">1</span>).astype(<span class="bu">int</span>)</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Split the data into training and testing sets</span></span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Train a logistic regression model</span></span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> LogisticRegression()</span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>model.fit(X_train, y_train)</span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Predict probabilities and classes</span></span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a>y_probs <span class="op">=</span> model.predict_proba(X_test)[:, <span class="dv">1</span>]</span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> model.predict(X_test)</span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a><span class="co"># ROC Curve and AUC</span></span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a>fpr, tpr, _ <span class="op">=</span> roc_curve(y_test, y_probs)</span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a>roc_auc <span class="op">=</span> auc(fpr, tpr)</span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Precision-Recall Curve</span></span>
<span id="cb7-27"><a href="#cb7-27" aria-hidden="true" tabindex="-1"></a>precision, recall, _ <span class="op">=</span> precision_recall_curve(y_test, y_probs)</span>
<span id="cb7-28"><a href="#cb7-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-29"><a href="#cb7-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Confusion Matrix</span></span>
<span id="cb7-30"><a href="#cb7-30" aria-hidden="true" tabindex="-1"></a>conf_matrix <span class="op">=</span> confusion_matrix(y_test, y_pred)</span>
<span id="cb7-31"><a href="#cb7-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-32"><a href="#cb7-32" aria-hidden="true" tabindex="-1"></a><span class="co"># Print or visualize metrics as needed</span></span>
<span id="cb7-33"><a href="#cb7-33" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Area under ROC Curve (AUC): </span><span class="sc">{</span>roc_auc<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb7-34"><a href="#cb7-34" aria-hidden="true" tabindex="-1"></a>plt.plot(fpr, tpr, color<span class="op">=</span><span class="st">'darkorange'</span>, lw<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'ROC Curve'</span>)</span>
<span id="cb7-35"><a href="#cb7-35" aria-hidden="true" tabindex="-1"></a>plt.plot([<span class="dv">0</span>, <span class="dv">1</span>], [<span class="dv">0</span>, <span class="dv">1</span>], color<span class="op">=</span><span class="st">'navy'</span>, lw<span class="op">=</span><span class="dv">2</span>, linestyle<span class="op">=</span><span class="st">'--'</span>)</span>
<span id="cb7-36"><a href="#cb7-36" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'False Positive Rate'</span>)</span>
<span id="cb7-37"><a href="#cb7-37" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'True Positive Rate'</span>)</span>
<span id="cb7-38"><a href="#cb7-38" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Receiver Operating Characteristic (ROC) Curve'</span>)</span>
<span id="cb7-39"><a href="#cb7-39" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb7-40"><a href="#cb7-40" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Area under ROC Curve (AUC): 1.0</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-7-output-2.png" width="589" height="449"></p>
</div>
</div>
<p>This code demonstrates a basic binary classification scenario with logistic regression and includes the calculation and visualization of ROC Curve, AUC, Precision-Recall Curve, and a Confusion Matrix. Adjustments can be made for multiclass classification scenarios.</p>
<div class="cell" data-execution_count="7">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> precision_recall_curve, auc</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Create example data</span></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.random.rand(<span class="dv">100</span>, <span class="dv">2</span>)</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> (X[:, <span class="dv">0</span>] <span class="op">+</span> X[:, <span class="dv">1</span>] <span class="op">&gt;</span> <span class="dv">1</span>).astype(<span class="bu">int</span>)</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Split the data into training and testing sets</span></span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Train a logistic regression model</span></span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> LogisticRegression()</span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a>model.fit(X_train, y_train)</span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Predict probabilities</span></span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a>y_probs <span class="op">=</span> model.predict_proba(X_test)[:, <span class="dv">1</span>]</span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Precision-Recall Curve</span></span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a>precision, recall, _ <span class="op">=</span> precision_recall_curve(y_test, y_probs)</span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a>pr_auc <span class="op">=</span> auc(recall, precision)</span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot Precision-Recall Curve</span></span>
<span id="cb9-26"><a href="#cb9-26" aria-hidden="true" tabindex="-1"></a>plt.plot(recall, precision, color<span class="op">=</span><span class="st">'blue'</span>, lw<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="ss">f'PR Curve (AUC = </span><span class="sc">{</span>pr_auc<span class="sc">:.2f}</span><span class="ss">)'</span>)</span>
<span id="cb9-27"><a href="#cb9-27" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Recall'</span>)</span>
<span id="cb9-28"><a href="#cb9-28" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Precision'</span>)</span>
<span id="cb9-29"><a href="#cb9-29" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Precision-Recall Curve'</span>)</span>
<span id="cb9-30"><a href="#cb9-30" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb9-31"><a href="#cb9-31" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-8-output-1.png" width="589" height="449"></p>
</div>
</div>
<p>The provided Python code generates a Precision-Recall (PR) Curve for a binary classification scenario. It uses a logistic regression model trained on a synthetic dataset. The dataset is split into training and testing sets, and the logistic regression model is fitted on the training data. The model’s predicted probabilities for the positive class on the test set are then used to plot the PR Curve. The curve illustrates the trade-off between precision and recall at various decision thresholds. The area under the PR Curve (AUC) is calculated and displayed in the plot, providing a quantitative measure of model performance.</p>
<div class="cell" data-execution_count="8">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> confusion_matrix</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Create example data</span></span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.random.rand(<span class="dv">100</span>, <span class="dv">2</span>)</span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> (X[:, <span class="dv">0</span>] <span class="op">+</span> X[:, <span class="dv">1</span>] <span class="op">&gt;</span> <span class="dv">1</span>).astype(<span class="bu">int</span>)</span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Split the data into training and testing sets</span></span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Train a logistic regression model</span></span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> LogisticRegression()</span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a>model.fit(X_train, y_train)</span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Predict classes</span></span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> model.predict(X_test)</span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Confusion Matrix</span></span>
<span id="cb10-23"><a href="#cb10-23" aria-hidden="true" tabindex="-1"></a>conf_matrix <span class="op">=</span> confusion_matrix(y_test, y_pred)</span>
<span id="cb10-24"><a href="#cb10-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-25"><a href="#cb10-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot Confusion Matrix</span></span>
<span id="cb10-26"><a href="#cb10-26" aria-hidden="true" tabindex="-1"></a>sns.heatmap(conf_matrix, annot<span class="op">=</span><span class="va">True</span>, fmt<span class="op">=</span><span class="st">'d'</span>, cmap<span class="op">=</span><span class="st">'Blues'</span>, cbar<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb10-27"><a href="#cb10-27" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Predicted Labels'</span>)</span>
<span id="cb10-28"><a href="#cb10-28" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'True Labels'</span>)</span>
<span id="cb10-29"><a href="#cb10-29" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Confusion Matrix'</span>)</span>
<span id="cb10-30"><a href="#cb10-30" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-9-output-1.png" width="581" height="449"></p>
</div>
</div>
<p>The presented Python code produces a Confusion Matrix for a binary classification task using a logistic regression model. Similar to the PR Curve code, it utilizes a synthetic dataset split into training and testing sets. The logistic regression model is trained on the training data, and its predictions on the test set are used to construct the confusion matrix. The matrix visualizes the model’s performance by showing the counts of true positive, true negative, false positive, and false negative predictions. The Seaborn library is employed to create a heatmap for clearer visualization of the confusion matrix. This allows for a detailed assessment of the model’s ability to correctly classify instances into positive and negative classes.</p>
<div class="cell" data-execution_count="9">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> confusion_matrix, roc_curve, auc, precision_recall_curve, average_precision_score</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score, precision_score, recall_score, f1_score</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Sample true labels and predicted labels</span></span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>y_true <span class="op">=</span> np.array([<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>])</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> np.array([<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">0</span>])</span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Get different types of confusion matrices</span></span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a>conf_matrix <span class="op">=</span> confusion_matrix(y_true, y_pred)</span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate additional metrics</span></span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a>accuracy <span class="op">=</span> accuracy_score(y_true, y_pred)</span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a>precision <span class="op">=</span> precision_score(y_true, y_pred)</span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a>recall <span class="op">=</span> recall_score(y_true, y_pred)</span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a>f1 <span class="op">=</span> f1_score(y_true, y_pred)</span>
<span id="cb11-19"><a href="#cb11-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-20"><a href="#cb11-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate ROC curve</span></span>
<span id="cb11-21"><a href="#cb11-21" aria-hidden="true" tabindex="-1"></a>fpr, tpr, _ <span class="op">=</span> roc_curve(y_true, y_pred)</span>
<span id="cb11-22"><a href="#cb11-22" aria-hidden="true" tabindex="-1"></a>roc_auc <span class="op">=</span> auc(fpr, tpr)</span>
<span id="cb11-23"><a href="#cb11-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-24"><a href="#cb11-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate Precision-Recall curve</span></span>
<span id="cb11-25"><a href="#cb11-25" aria-hidden="true" tabindex="-1"></a>precision_curve, recall_curve, _ <span class="op">=</span> precision_recall_curve(y_true, y_pred)</span>
<span id="cb11-26"><a href="#cb11-26" aria-hidden="true" tabindex="-1"></a>pr_auc <span class="op">=</span> average_precision_score(y_true, y_pred)</span>
<span id="cb11-27"><a href="#cb11-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-28"><a href="#cb11-28" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a figure with subplots</span></span>
<span id="cb11-29"><a href="#cb11-29" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">4</span>, figsize<span class="op">=</span>(<span class="dv">15</span>, <span class="dv">4</span>))</span>
<span id="cb11-30"><a href="#cb11-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-31"><a href="#cb11-31" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot confusion matrices</span></span>
<span id="cb11-32"><a href="#cb11-32" aria-hidden="true" tabindex="-1"></a>sns.heatmap(conf_matrix, annot<span class="op">=</span><span class="va">True</span>, fmt<span class="op">=</span><span class="st">"d"</span>, cmap<span class="op">=</span><span class="st">"Blues"</span>, cbar<span class="op">=</span><span class="va">False</span>, ax<span class="op">=</span>axes[<span class="dv">0</span>])</span>
<span id="cb11-33"><a href="#cb11-33" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_title(<span class="st">'Confusion Matrix'</span>)</span>
<span id="cb11-34"><a href="#cb11-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-35"><a href="#cb11-35" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot additional metrics</span></span>
<span id="cb11-36"><a href="#cb11-36" aria-hidden="true" tabindex="-1"></a>sns.barplot(x<span class="op">=</span>[<span class="st">'Accuracy'</span>, <span class="st">'Precision'</span>, <span class="st">'Recall'</span>, <span class="st">'F1 Score'</span>], y<span class="op">=</span>[accuracy, precision, recall, f1], ax<span class="op">=</span>axes[<span class="dv">1</span>])</span>
<span id="cb11-37"><a href="#cb11-37" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_title(<span class="st">'Additional Metrics'</span>)</span>
<span id="cb11-38"><a href="#cb11-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-39"><a href="#cb11-39" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot ROC Curve</span></span>
<span id="cb11-40"><a href="#cb11-40" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">2</span>].plot(fpr, tpr, color<span class="op">=</span><span class="st">'darkorange'</span>, lw<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'ROC curve (area = </span><span class="sc">{:.2f}</span><span class="st">)'</span>.<span class="bu">format</span>(roc_auc))</span>
<span id="cb11-41"><a href="#cb11-41" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">2</span>].plot([<span class="dv">0</span>, <span class="dv">1</span>], [<span class="dv">0</span>, <span class="dv">1</span>], color<span class="op">=</span><span class="st">'navy'</span>, lw<span class="op">=</span><span class="dv">2</span>, linestyle<span class="op">=</span><span class="st">'--'</span>)</span>
<span id="cb11-42"><a href="#cb11-42" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">2</span>].set_xlim([<span class="fl">0.0</span>, <span class="fl">1.0</span>])</span>
<span id="cb11-43"><a href="#cb11-43" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">2</span>].set_ylim([<span class="fl">0.0</span>, <span class="fl">1.05</span>])</span>
<span id="cb11-44"><a href="#cb11-44" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">2</span>].set_xlabel(<span class="st">'False Positive Rate'</span>)</span>
<span id="cb11-45"><a href="#cb11-45" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">2</span>].set_ylabel(<span class="st">'True Positive Rate'</span>)</span>
<span id="cb11-46"><a href="#cb11-46" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">2</span>].set_title(<span class="st">'ROC Curve'</span>)</span>
<span id="cb11-47"><a href="#cb11-47" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">2</span>].legend(loc<span class="op">=</span><span class="st">"lower right"</span>)</span>
<span id="cb11-48"><a href="#cb11-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-49"><a href="#cb11-49" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot Precision-Recall Curve</span></span>
<span id="cb11-50"><a href="#cb11-50" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">3</span>].plot(recall_curve, precision_curve, color<span class="op">=</span><span class="st">'darkorange'</span>, lw<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'Precision-Recall curve (area = </span><span class="sc">{:.2f}</span><span class="st">)'</span>.<span class="bu">format</span>(pr_auc))</span>
<span id="cb11-51"><a href="#cb11-51" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">3</span>].set_xlim([<span class="fl">0.0</span>, <span class="fl">1.0</span>])</span>
<span id="cb11-52"><a href="#cb11-52" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">3</span>].set_ylim([<span class="fl">0.0</span>, <span class="fl">1.05</span>])</span>
<span id="cb11-53"><a href="#cb11-53" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">3</span>].set_xlabel(<span class="st">'Recall'</span>)</span>
<span id="cb11-54"><a href="#cb11-54" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">3</span>].set_ylabel(<span class="st">'Precision'</span>)</span>
<span id="cb11-55"><a href="#cb11-55" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">3</span>].set_title(<span class="st">'Precision-Recall Curve'</span>)</span>
<span id="cb11-56"><a href="#cb11-56" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">3</span>].legend(loc<span class="op">=</span><span class="st">"lower right"</span>)</span>
<span id="cb11-57"><a href="#cb11-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-58"><a href="#cb11-58" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb11-59"><a href="#cb11-59" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-10-output-1.png" width="1430" height="374"></p>
</div>
</div>
<p>This code uses a sample set of true labels (y_true) and predicted labels (y_pred). It calculates the confusion matrix and additional metrics such as accuracy, precision, recall, and F1 score. The results are visualized using Seaborn in a side-by-side format with subplots for the confusion matrix, additional metrics, ROC Curve, and Precision-Recall Curve. Make sure to adjust the labels and data according to your specific use case.</p>
</section>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion"><strong>Conclusion:</strong></h2>
<p>Classification algorithms are indispensable tools for extracting patterns and making predictions across a spectrum of applications. The Python examples, complemented by advanced plots and evaluation metrics, provide a glimpse into the richness and versatility of classification in machine learning. As we navigate the landscape of classification, these techniques empower us to unravel complex relationships and contribute meaningfully to data-driven decision-making.</p>


<!-- -->

</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../../posts/4.0 Linear and non-linear regression/index.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">4. Linear and Non-Linear Regression</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../posts/6.0 Anomaly Outlier detection/index.html" class="pagination-link">
        <span class="nav-page-text">6. Anomaly Outlier Detection</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb12" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "5\\. Classification"</span></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="an">author:</span><span class="co"> "M Mubashar Ashraf"</span></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a><span class="an">date:</span><span class="co"> "2023-11-21"</span></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a><span class="an">categories:</span><span class="co"> [Classification]</span></span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a><span class="an">output:</span><span class="co"> html_document</span></span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a><span class="an">image:</span><span class="co"> "C.png"</span></span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a><span class="an">format:</span></span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a><span class="co">  html:</span></span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a><span class="co">    code-fold: true</span></span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a><span class="an">jupyter:</span><span class="co"> python3</span></span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a><span class="an">code-fold:</span><span class="co"> true</span></span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a><span class="an">keep-ipynb:</span><span class="co"> true</span></span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a>**Introduction:** Classification, a fundamental task in machine learning, involves assigning predefined labels to instances based on their features. In this exploration, we delve into the intricacies of classification algorithms, showcasing their applications through Python examples. The journey will unravel the methodologies behind several classification techniques and demonstrate how to leverage them for insightful predictions.</span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a><span class="fu">## **Binary Classification: Discriminating Between Two Classes**</span></span>
<span id="cb12-19"><a href="#cb12-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-20"><a href="#cb12-20" aria-hidden="true" tabindex="-1"></a><span class="fu">### **Logistic Regression:**</span></span>
<span id="cb12-21"><a href="#cb12-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-22"><a href="#cb12-22" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>Logistic regression is a type of statistical method used in machine learning for binary classification problems.</span>
<span id="cb12-23"><a href="#cb12-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-24"><a href="#cb12-24" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>It's a go-to algorithm when the outcome we want to predict falls into one of two categories, like whether an email is spam or not.</span>
<span id="cb12-25"><a href="#cb12-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-26"><a href="#cb12-26" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>Unlike linear regression, which predicts continuous values, logistic regression predicts the probability that an instance belongs to a particular category.</span>
<span id="cb12-27"><a href="#cb12-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-28"><a href="#cb12-28" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>The predicted probability is then transformed using a logistic function, squashing it between 0 and 1.</span>
<span id="cb12-29"><a href="#cb12-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-30"><a href="#cb12-30" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>The outcome is classified based on a chosen threshold: if the probability is above the threshold, it's one class; otherwise, it's the other.</span>
<span id="cb12-31"><a href="#cb12-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-32"><a href="#cb12-32" aria-hidden="true" tabindex="-1"></a>**Key Features:**</span>
<span id="cb12-33"><a href="#cb12-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-34"><a href="#cb12-34" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>**Probabilistic Prediction:** Logistic regression predicts the likelihood (probability) of an instance belonging to a specific class.</span>
<span id="cb12-35"><a href="#cb12-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-36"><a href="#cb12-36" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>**Sigmoid Function:** The logistic or sigmoid function is used to convert raw predictions into probabilities, ensuring they're within the 0 to 1 range.</span>
<span id="cb12-37"><a href="#cb12-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-38"><a href="#cb12-38" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>**Binary Outcome:** Ideal for scenarios where the outcome is binary, like yes/no, spam/not spam, etc.</span>
<span id="cb12-39"><a href="#cb12-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-40"><a href="#cb12-40" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>**Simple and Interpretable:** Logistic regression is relatively simple, easy to understand, and provides interpretable results.</span>
<span id="cb12-41"><a href="#cb12-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-42"><a href="#cb12-42" aria-hidden="true" tabindex="-1"></a>**Applications:**</span>
<span id="cb12-43"><a href="#cb12-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-44"><a href="#cb12-44" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>**Spam Detection:** Identifying whether an email is spam or not.</span>
<span id="cb12-45"><a href="#cb12-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-46"><a href="#cb12-46" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>**Medical Diagnosis:** Predicting if a patient has a particular medical condition based on test results.</span>
<span id="cb12-47"><a href="#cb12-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-48"><a href="#cb12-48" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>**Credit Scoring:** Assessing the likelihood of a customer defaulting on a loan.</span>
<span id="cb12-49"><a href="#cb12-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-50"><a href="#cb12-50" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>**Customer Churn:** Predicting whether a customer is likely to leave a service or not.</span>
<span id="cb12-51"><a href="#cb12-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-52"><a href="#cb12-52" aria-hidden="true" tabindex="-1"></a>Logistic Regression is a go-to algorithm for binary classification tasks.</span>
<span id="cb12-53"><a href="#cb12-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-56"><a href="#cb12-56" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb12-57"><a href="#cb12-57" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb12-58"><a href="#cb12-58" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb12-59"><a href="#cb12-59" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_classification</span>
<span id="cb12-60"><a href="#cb12-60" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</span>
<span id="cb12-61"><a href="#cb12-61" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb12-62"><a href="#cb12-62" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score, confusion_matrix</span>
<span id="cb12-63"><a href="#cb12-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-64"><a href="#cb12-64" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate synthetic data for binary classification</span></span>
<span id="cb12-65"><a href="#cb12-65" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> make_classification(n_samples<span class="op">=</span><span class="dv">300</span>, n_features<span class="op">=</span><span class="dv">2</span>, n_informative<span class="op">=</span><span class="dv">2</span>, n_redundant<span class="op">=</span><span class="dv">0</span>, n_clusters_per_class<span class="op">=</span><span class="dv">1</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb12-66"><a href="#cb12-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-67"><a href="#cb12-67" aria-hidden="true" tabindex="-1"></a><span class="co"># Split data into training and testing sets</span></span>
<span id="cb12-68"><a href="#cb12-68" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.25</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb12-69"><a href="#cb12-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-70"><a href="#cb12-70" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit a logistic regression model</span></span>
<span id="cb12-71"><a href="#cb12-71" aria-hidden="true" tabindex="-1"></a>model_logistic <span class="op">=</span> LogisticRegression()</span>
<span id="cb12-72"><a href="#cb12-72" aria-hidden="true" tabindex="-1"></a>model_logistic.fit(X_train, y_train)</span>
<span id="cb12-73"><a href="#cb12-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-74"><a href="#cb12-74" aria-hidden="true" tabindex="-1"></a><span class="co"># Predictions and evaluation</span></span>
<span id="cb12-75"><a href="#cb12-75" aria-hidden="true" tabindex="-1"></a>y_pred_logistic <span class="op">=</span> model_logistic.predict(X_test)</span>
<span id="cb12-76"><a href="#cb12-76" aria-hidden="true" tabindex="-1"></a>accuracy_logistic <span class="op">=</span> accuracy_score(y_test, y_pred_logistic)</span>
<span id="cb12-77"><a href="#cb12-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-78"><a href="#cb12-78" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize decision boundary for logistic regression</span></span>
<span id="cb12-79"><a href="#cb12-79" aria-hidden="true" tabindex="-1"></a>plt.scatter(X[:, <span class="dv">0</span>], X[:, <span class="dv">1</span>], c<span class="op">=</span>y, cmap<span class="op">=</span>plt.cm.Paired, edgecolors<span class="op">=</span><span class="st">'k'</span>)</span>
<span id="cb12-80"><a href="#cb12-80" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Logistic Regression - Decision Boundary'</span>)</span>
<span id="cb12-81"><a href="#cb12-81" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Feature 1'</span>)</span>
<span id="cb12-82"><a href="#cb12-82" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Feature 2'</span>)</span>
<span id="cb12-83"><a href="#cb12-83" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb12-84"><a href="#cb12-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-85"><a href="#cb12-85" aria-hidden="true" tabindex="-1"></a><span class="co"># Display confusion matrix for logistic regression</span></span>
<span id="cb12-86"><a href="#cb12-86" aria-hidden="true" tabindex="-1"></a>conf_matrix_logistic <span class="op">=</span> confusion_matrix(y_test, y_pred_logistic)</span>
<span id="cb12-87"><a href="#cb12-87" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Logistic Regression Example:"</span>)</span>
<span id="cb12-88"><a href="#cb12-88" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Confusion Matrix:</span><span class="ch">\n</span><span class="st">"</span>, conf_matrix_logistic)</span>
<span id="cb12-89"><a href="#cb12-89" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Accuracy: </span><span class="sc">{</span>accuracy_logistic<span class="sc">:.2%}</span><span class="ss">"</span>)</span>
<span id="cb12-90"><a href="#cb12-90" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Explanation: Logistic Regression applied to binary classification with a visual representation of the decision boundary."</span>)</span>
<span id="cb12-91"><a href="#cb12-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-92"><a href="#cb12-92" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb12-93"><a href="#cb12-93" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-94"><a href="#cb12-94" aria-hidden="true" tabindex="-1"></a><span class="fu">### **Support Vector Machines (SVM):**</span></span>
<span id="cb12-95"><a href="#cb12-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-96"><a href="#cb12-96" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>SVM is a versatile and powerful machine learning algorithm used for both classification and regression tasks.</span>
<span id="cb12-97"><a href="#cb12-97" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-98"><a href="#cb12-98" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>Its primary goal is to find the optimal hyperplane that best separates different classes in the data.</span>
<span id="cb12-99"><a href="#cb12-99" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-100"><a href="#cb12-100" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>SVM is especially effective in high-dimensional spaces and is robust even when the number of features exceeds the number of samples.</span>
<span id="cb12-101"><a href="#cb12-101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-102"><a href="#cb12-102" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>It's a supervised learning algorithm, meaning it learns from labeled training data to make predictions on unseen data.</span>
<span id="cb12-103"><a href="#cb12-103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-104"><a href="#cb12-104" aria-hidden="true" tabindex="-1"></a>**Key Features:**</span>
<span id="cb12-105"><a href="#cb12-105" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-106"><a href="#cb12-106" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>**Maximizing Margin:** SVM aims to find a hyperplane with the maximum margin, the distance between the hyperplane and the nearest data point from each class.</span>
<span id="cb12-107"><a href="#cb12-107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-108"><a href="#cb12-108" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>**Support Vectors:** The critical data points that determine the position of the optimal hyperplane are called support vectors.</span>
<span id="cb12-109"><a href="#cb12-109" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-110"><a href="#cb12-110" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>**Kernel Trick:** SVM can handle non-linear relationships between features by using a kernel function to map the data into a higher-dimensional space.</span>
<span id="cb12-111"><a href="#cb12-111" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-112"><a href="#cb12-112" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>**Versatility:** SVM can be used for binary and multiclass classification as well as regression tasks.</span>
<span id="cb12-113"><a href="#cb12-113" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-114"><a href="#cb12-114" aria-hidden="true" tabindex="-1"></a>**Applications:**</span>
<span id="cb12-115"><a href="#cb12-115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-116"><a href="#cb12-116" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>**Image Classification:** SVM is used in image recognition tasks to classify objects in images.</span>
<span id="cb12-117"><a href="#cb12-117" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-118"><a href="#cb12-118" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>**Text Classification:** It's employed in natural language processing for tasks like spam detection and sentiment analysis.</span>
<span id="cb12-119"><a href="#cb12-119" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-120"><a href="#cb12-120" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>**Bioinformatics:** SVM helps in classifying biological data, such as predicting protein functions.</span>
<span id="cb12-121"><a href="#cb12-121" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-122"><a href="#cb12-122" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>**Finance:** SVM is used in predicting stock prices and credit scoring.</span>
<span id="cb12-123"><a href="#cb12-123" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-124"><a href="#cb12-124" aria-hidden="true" tabindex="-1"></a>Support Vector Machines are valued for their ability to handle complex relationships in data and find robust decision boundaries, making them applicable to various real-world problems.</span>
<span id="cb12-125"><a href="#cb12-125" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-126"><a href="#cb12-126" aria-hidden="true" tabindex="-1"></a>Support Vector Machines offer robust performance in binary classification scenarios.</span>
<span id="cb12-127"><a href="#cb12-127" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-130"><a href="#cb12-130" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb12-131"><a href="#cb12-131" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb12-132"><a href="#cb12-132" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb12-133"><a href="#cb12-133" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb12-134"><a href="#cb12-134" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb12-135"><a href="#cb12-135" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> generate_random_dataset(size):</span>
<span id="cb12-136"><a href="#cb12-136" aria-hidden="true" tabindex="-1"></a>    <span class="co">""" Generate a random dataset and that follows a quadratic  distribution</span></span>
<span id="cb12-137"><a href="#cb12-137" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb12-138"><a href="#cb12-138" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> []</span>
<span id="cb12-139"><a href="#cb12-139" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> []</span>
<span id="cb12-140"><a href="#cb12-140" aria-hidden="true" tabindex="-1"></a>    target <span class="op">=</span> []</span>
<span id="cb12-141"><a href="#cb12-141" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(size):</span>
<span id="cb12-142"><a href="#cb12-142" aria-hidden="true" tabindex="-1"></a>        <span class="co"># class zero</span></span>
<span id="cb12-143"><a href="#cb12-143" aria-hidden="true" tabindex="-1"></a>        x.append(np.<span class="bu">round</span>(random.uniform(<span class="dv">0</span>, <span class="fl">2.5</span>), <span class="dv">1</span>))</span>
<span id="cb12-144"><a href="#cb12-144" aria-hidden="true" tabindex="-1"></a>        y.append(np.<span class="bu">round</span>(random.uniform(<span class="dv">0</span>, <span class="dv">20</span>), <span class="dv">1</span>))</span>
<span id="cb12-145"><a href="#cb12-145" aria-hidden="true" tabindex="-1"></a>        target.append(<span class="dv">0</span>)</span>
<span id="cb12-146"><a href="#cb12-146" aria-hidden="true" tabindex="-1"></a>        <span class="co"># class one</span></span>
<span id="cb12-147"><a href="#cb12-147" aria-hidden="true" tabindex="-1"></a>        x.append(np.<span class="bu">round</span>(random.uniform(<span class="dv">1</span>, <span class="dv">5</span>), <span class="dv">2</span>))</span>
<span id="cb12-148"><a href="#cb12-148" aria-hidden="true" tabindex="-1"></a>        y.append(np.<span class="bu">round</span>(random.uniform(<span class="dv">20</span>, <span class="dv">25</span>), <span class="dv">2</span>))</span>
<span id="cb12-149"><a href="#cb12-149" aria-hidden="true" tabindex="-1"></a>        target.append(<span class="dv">1</span>)</span>
<span id="cb12-150"><a href="#cb12-150" aria-hidden="true" tabindex="-1"></a>        x.append(np.<span class="bu">round</span>(random.uniform(<span class="dv">3</span>, <span class="dv">5</span>), <span class="dv">2</span>))</span>
<span id="cb12-151"><a href="#cb12-151" aria-hidden="true" tabindex="-1"></a>        y.append(np.<span class="bu">round</span>(random.uniform(<span class="dv">5</span>, <span class="dv">25</span>), <span class="dv">2</span>))</span>
<span id="cb12-152"><a href="#cb12-152" aria-hidden="true" tabindex="-1"></a>        target.append(<span class="dv">1</span>)</span>
<span id="cb12-153"><a href="#cb12-153" aria-hidden="true" tabindex="-1"></a>    df_x <span class="op">=</span> pd.DataFrame(data<span class="op">=</span>x)</span>
<span id="cb12-154"><a href="#cb12-154" aria-hidden="true" tabindex="-1"></a>    df_y <span class="op">=</span> pd.DataFrame(data<span class="op">=</span>y)</span>
<span id="cb12-155"><a href="#cb12-155" aria-hidden="true" tabindex="-1"></a>    df_target <span class="op">=</span> pd.DataFrame(data<span class="op">=</span>target)</span>
<span id="cb12-156"><a href="#cb12-156" aria-hidden="true" tabindex="-1"></a>    data_frame <span class="op">=</span> pd.concat([df_x, df_y], ignore_index<span class="op">=</span><span class="va">True</span>, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb12-157"><a href="#cb12-157" aria-hidden="true" tabindex="-1"></a>    data_frame <span class="op">=</span> pd.concat([data_frame, df_target], ignore_index<span class="op">=</span><span class="va">True</span>, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb12-158"><a href="#cb12-158" aria-hidden="true" tabindex="-1"></a>    data_frame.columns <span class="op">=</span> [<span class="st">'x'</span>, <span class="st">'y'</span>, <span class="st">'target'</span>]</span>
<span id="cb12-159"><a href="#cb12-159" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> data_frame</span>
<span id="cb12-160"><a href="#cb12-160" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-161"><a href="#cb12-161" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate dataset</span></span>
<span id="cb12-162"><a href="#cb12-162" aria-hidden="true" tabindex="-1"></a>size <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb12-163"><a href="#cb12-163" aria-hidden="true" tabindex="-1"></a>dataset <span class="op">=</span> generate_random_dataset(size)</span>
<span id="cb12-164"><a href="#cb12-164" aria-hidden="true" tabindex="-1"></a>features <span class="op">=</span> dataset[[<span class="st">'x'</span>, <span class="st">'y'</span>]]</span>
<span id="cb12-165"><a href="#cb12-165" aria-hidden="true" tabindex="-1"></a>label <span class="op">=</span> dataset[<span class="st">'target'</span>]</span>
<span id="cb12-166"><a href="#cb12-166" aria-hidden="true" tabindex="-1"></a><span class="co"># Hold out 20% of the dataset for training</span></span>
<span id="cb12-167"><a href="#cb12-167" aria-hidden="true" tabindex="-1"></a>test_size <span class="op">=</span> <span class="bu">int</span>(np.<span class="bu">round</span>(size <span class="op">*</span> <span class="fl">0.2</span>, <span class="dv">0</span>))</span>
<span id="cb12-168"><a href="#cb12-168" aria-hidden="true" tabindex="-1"></a><span class="co"># Split dataset into training and testing sets</span></span>
<span id="cb12-169"><a href="#cb12-169" aria-hidden="true" tabindex="-1"></a>x_train <span class="op">=</span> features[:<span class="op">-</span>test_size].values</span>
<span id="cb12-170"><a href="#cb12-170" aria-hidden="true" tabindex="-1"></a>y_train <span class="op">=</span> label[:<span class="op">-</span>test_size].values</span>
<span id="cb12-171"><a href="#cb12-171" aria-hidden="true" tabindex="-1"></a>x_test <span class="op">=</span> features[<span class="op">-</span>test_size:].values</span>
<span id="cb12-172"><a href="#cb12-172" aria-hidden="true" tabindex="-1"></a>y_test <span class="op">=</span> label[<span class="op">-</span>test_size:].values</span>
<span id="cb12-173"><a href="#cb12-173" aria-hidden="true" tabindex="-1"></a><span class="co"># Plotting the training set</span></span>
<span id="cb12-174"><a href="#cb12-174" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">7</span>))</span>
<span id="cb12-175"><a href="#cb12-175" aria-hidden="true" tabindex="-1"></a><span class="co"># removing to and right border</span></span>
<span id="cb12-176"><a href="#cb12-176" aria-hidden="true" tabindex="-1"></a>ax.spines[<span class="st">'top'</span>].set_visible(<span class="va">False</span>)</span>
<span id="cb12-177"><a href="#cb12-177" aria-hidden="true" tabindex="-1"></a>ax.spines[<span class="st">'left'</span>].set_visible(<span class="va">False</span>)</span>
<span id="cb12-178"><a href="#cb12-178" aria-hidden="true" tabindex="-1"></a>ax.spines[<span class="st">'right'</span>].set_visible(<span class="va">False</span>)</span>
<span id="cb12-179"><a href="#cb12-179" aria-hidden="true" tabindex="-1"></a><span class="co"># adding major gridlines</span></span>
<span id="cb12-180"><a href="#cb12-180" aria-hidden="true" tabindex="-1"></a>ax.grid(color<span class="op">=</span><span class="st">'grey'</span>, linestyle<span class="op">=</span><span class="st">'-'</span>, linewidth<span class="op">=</span><span class="fl">0.25</span>, alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb12-181"><a href="#cb12-181" aria-hidden="true" tabindex="-1"></a>ax.scatter(features[:<span class="op">-</span>test_size][<span class="st">'x'</span>], features[:<span class="op">-</span>test_size][<span class="st">'y'</span>], color<span class="op">=</span><span class="st">"#8C7298"</span>)</span>
<span id="cb12-182"><a href="#cb12-182" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb12-183"><a href="#cb12-183" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb12-184"><a href="#cb12-184" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-185"><a href="#cb12-185" aria-hidden="true" tabindex="-1"></a><span class="fu">## **Multiclass Classification: Navigating Diverse Class Labels**</span></span>
<span id="cb12-186"><a href="#cb12-186" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-187"><a href="#cb12-187" aria-hidden="true" tabindex="-1"></a><span class="fu">### **Random Forest:**</span></span>
<span id="cb12-188"><a href="#cb12-188" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-189"><a href="#cb12-189" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>Random Forest is a powerful ensemble learning algorithm used for both classification and regression tasks.</span>
<span id="cb12-190"><a href="#cb12-190" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-191"><a href="#cb12-191" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>It operates by constructing multiple decision trees during training and outputs the mode of the classes (classification) or the mean prediction (regression) of the individual trees.</span>
<span id="cb12-192"><a href="#cb12-192" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-193"><a href="#cb12-193" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>It's an ensemble method, meaning it combines the predictions of multiple models to improve overall accuracy and robustness.</span>
<span id="cb12-194"><a href="#cb12-194" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-195"><a href="#cb12-195" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>The "forest" in Random Forest is a collection of decision trees, and the "random" part comes from the introduction of randomness during both training and prediction phases.</span>
<span id="cb12-196"><a href="#cb12-196" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-197"><a href="#cb12-197" aria-hidden="true" tabindex="-1"></a>**Key Features:**</span>
<span id="cb12-198"><a href="#cb12-198" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-199"><a href="#cb12-199" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>**Decision Trees:** Random Forest is built on the foundation of decision trees, which are known for their simplicity and interpretability.</span>
<span id="cb12-200"><a href="#cb12-200" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-201"><a href="#cb12-201" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>**Bootstrap Aggregating (Bagging):** Random Forest employs bagging, a technique that involves training each tree on a random subset of the data, allowing for diverse trees.</span>
<span id="cb12-202"><a href="#cb12-202" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-203"><a href="#cb12-203" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>**Feature Randomness:** During the construction of each tree, a random subset of features is considered at each split, enhancing the model's diversity.</span>
<span id="cb12-204"><a href="#cb12-204" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-205"><a href="#cb12-205" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>**Reducing Overfitting:** The combination of diverse trees and feature randomness helps mitigate overfitting, making Random Forest robust to noisy data.</span>
<span id="cb12-206"><a href="#cb12-206" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-207"><a href="#cb12-207" aria-hidden="true" tabindex="-1"></a>**Applications:**</span>
<span id="cb12-208"><a href="#cb12-208" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-209"><a href="#cb12-209" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>**Image Classification:** Random Forest is used in image recognition tasks to classify objects in images.</span>
<span id="cb12-210"><a href="#cb12-210" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-211"><a href="#cb12-211" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>**Healthcare:** It's applied in predicting diseases based on patient data and medical images.</span>
<span id="cb12-212"><a href="#cb12-212" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-213"><a href="#cb12-213" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>**Finance:** Random Forest is employed in credit scoring and fraud detection.</span>
<span id="cb12-214"><a href="#cb12-214" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-215"><a href="#cb12-215" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>**Ecology:** In ecology, it helps analyze and predict biodiversity patterns.</span>
<span id="cb12-216"><a href="#cb12-216" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-217"><a href="#cb12-217" aria-hidden="true" tabindex="-1"></a>Random Forest is valued for its ability to handle complex relationships in data, provide robust predictions, and mitigate overfitting, making it a widely used algorithm in various domains.</span>
<span id="cb12-218"><a href="#cb12-218" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-219"><a href="#cb12-219" aria-hidden="true" tabindex="-1"></a>Random Forest is an ensemble method known for its versatility in handling multiclass classification.</span>
<span id="cb12-220"><a href="#cb12-220" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-223"><a href="#cb12-223" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb12-224"><a href="#cb12-224" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb12-225"><a href="#cb12-225" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> load_breast_cancer</span>
<span id="cb12-226"><a href="#cb12-226" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn <span class="im">import</span> tree</span>
<span id="cb12-227"><a href="#cb12-227" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb12-228"><a href="#cb12-228" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> RandomForestClassifier</span>
<span id="cb12-229"><a href="#cb12-229" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb12-230"><a href="#cb12-230" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-231"><a href="#cb12-231" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the Breast Cancer Dataset</span></span>
<span id="cb12-232"><a href="#cb12-232" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> load_breast_cancer()</span>
<span id="cb12-233"><a href="#cb12-233" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.DataFrame(data.data, columns<span class="op">=</span>data.feature_names)</span>
<span id="cb12-234"><a href="#cb12-234" aria-hidden="true" tabindex="-1"></a>df[<span class="st">'target'</span>] <span class="op">=</span> data.target</span>
<span id="cb12-235"><a href="#cb12-235" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-236"><a href="#cb12-236" aria-hidden="true" tabindex="-1"></a><span class="co"># Arrange Data into Features Matrix and Target Vector</span></span>
<span id="cb12-237"><a href="#cb12-237" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> df.loc[:, df.columns <span class="op">!=</span> <span class="st">'target'</span>]</span>
<span id="cb12-238"><a href="#cb12-238" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> df.loc[:, <span class="st">'target'</span>].values</span>
<span id="cb12-239"><a href="#cb12-239" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-240"><a href="#cb12-240" aria-hidden="true" tabindex="-1"></a><span class="co"># Split the data into training and testing sets</span></span>
<span id="cb12-241"><a href="#cb12-241" aria-hidden="true" tabindex="-1"></a>X_train, X_test, Y_train, Y_test <span class="op">=</span> train_test_split(X, y, random_state<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb12-242"><a href="#cb12-242" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-243"><a href="#cb12-243" aria-hidden="true" tabindex="-1"></a><span class="co"># Random Forests in `scikit-learn` (with N = 100)</span></span>
<span id="cb12-244"><a href="#cb12-244" aria-hidden="true" tabindex="-1"></a>rf <span class="op">=</span> RandomForestClassifier(n_estimators<span class="op">=</span><span class="dv">100</span>,</span>
<span id="cb12-245"><a href="#cb12-245" aria-hidden="true" tabindex="-1"></a>                            random_state<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb12-246"><a href="#cb12-246" aria-hidden="true" tabindex="-1"></a>rf.fit(X_train, Y_train)</span>
<span id="cb12-247"><a href="#cb12-247" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-248"><a href="#cb12-248" aria-hidden="true" tabindex="-1"></a>fn<span class="op">=</span>data.feature_names</span>
<span id="cb12-249"><a href="#cb12-249" aria-hidden="true" tabindex="-1"></a>cn<span class="op">=</span>data.target_names</span>
<span id="cb12-250"><a href="#cb12-250" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(nrows <span class="op">=</span> <span class="dv">1</span>,ncols <span class="op">=</span> <span class="dv">1</span>,figsize <span class="op">=</span> (<span class="dv">4</span>,<span class="dv">4</span>), dpi<span class="op">=</span><span class="dv">800</span>)</span>
<span id="cb12-251"><a href="#cb12-251" aria-hidden="true" tabindex="-1"></a>tree.plot_tree(rf.estimators_[<span class="dv">0</span>],</span>
<span id="cb12-252"><a href="#cb12-252" aria-hidden="true" tabindex="-1"></a>               feature_names <span class="op">=</span> fn, </span>
<span id="cb12-253"><a href="#cb12-253" aria-hidden="true" tabindex="-1"></a>               class_names<span class="op">=</span>cn,</span>
<span id="cb12-254"><a href="#cb12-254" aria-hidden="true" tabindex="-1"></a>               filled <span class="op">=</span> <span class="va">True</span>)<span class="op">;</span></span>
<span id="cb12-255"><a href="#cb12-255" aria-hidden="true" tabindex="-1"></a>fig.savefig(<span class="st">'rf_individualtree.png'</span>)</span>
<span id="cb12-256"><a href="#cb12-256" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb12-257"><a href="#cb12-257" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-258"><a href="#cb12-258" aria-hidden="true" tabindex="-1"></a><span class="fu">## **Evaluation Metrics and Beyond**</span></span>
<span id="cb12-259"><a href="#cb12-259" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-260"><a href="#cb12-260" aria-hidden="true" tabindex="-1"></a><span class="fu">### **Receiver Operating Characteristic (ROC) Curve:**</span></span>
<span id="cb12-261"><a href="#cb12-261" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-262"><a href="#cb12-262" aria-hidden="true" tabindex="-1"></a>ROC curves visualize the trade-off between true positive rate and false positive rate.</span>
<span id="cb12-263"><a href="#cb12-263" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-266"><a href="#cb12-266" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb12-267"><a href="#cb12-267" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb12-268"><a href="#cb12-268" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb12-269"><a href="#cb12-269" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> load_breast_cancer</span>
<span id="cb12-270"><a href="#cb12-270" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb12-271"><a href="#cb12-271" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.svm <span class="im">import</span> SVC</span>
<span id="cb12-272"><a href="#cb12-272" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> roc_curve, auc</span>
<span id="cb12-273"><a href="#cb12-273" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-274"><a href="#cb12-274" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the Breast Cancer dataset</span></span>
<span id="cb12-275"><a href="#cb12-275" aria-hidden="true" tabindex="-1"></a>cancer <span class="op">=</span> load_breast_cancer()</span>
<span id="cb12-276"><a href="#cb12-276" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> cancer.data</span>
<span id="cb12-277"><a href="#cb12-277" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> cancer.target</span>
<span id="cb12-278"><a href="#cb12-278" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-279"><a href="#cb12-279" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert the problem to binary classification (class 0 vs class 1)</span></span>
<span id="cb12-280"><a href="#cb12-280" aria-hidden="true" tabindex="-1"></a>y_binary <span class="op">=</span> (y <span class="op">==</span> <span class="dv">0</span>).astype(<span class="bu">int</span>)</span>
<span id="cb12-281"><a href="#cb12-281" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-282"><a href="#cb12-282" aria-hidden="true" tabindex="-1"></a><span class="co"># Split the data into training and testing sets</span></span>
<span id="cb12-283"><a href="#cb12-283" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y_binary, test_size<span class="op">=</span><span class="fl">0.25</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb12-284"><a href="#cb12-284" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-285"><a href="#cb12-285" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a simple SVM model for binary classification</span></span>
<span id="cb12-286"><a href="#cb12-286" aria-hidden="true" tabindex="-1"></a>svm_model <span class="op">=</span> SVC(kernel<span class="op">=</span><span class="st">'linear'</span>, probability<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb12-287"><a href="#cb12-287" aria-hidden="true" tabindex="-1"></a>svm_model.fit(X_train, y_train)</span>
<span id="cb12-288"><a href="#cb12-288" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-289"><a href="#cb12-289" aria-hidden="true" tabindex="-1"></a><span class="co"># Obtain class probabilities for positive class (class 0)</span></span>
<span id="cb12-290"><a href="#cb12-290" aria-hidden="true" tabindex="-1"></a>y_probabilities <span class="op">=</span> svm_model.predict_proba(X_test)[:, <span class="dv">1</span>]</span>
<span id="cb12-291"><a href="#cb12-291" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-292"><a href="#cb12-292" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute ROC curve and AUC</span></span>
<span id="cb12-293"><a href="#cb12-293" aria-hidden="true" tabindex="-1"></a>fpr, tpr, _ <span class="op">=</span> roc_curve(y_test, y_probabilities)</span>
<span id="cb12-294"><a href="#cb12-294" aria-hidden="true" tabindex="-1"></a>roc_auc <span class="op">=</span> auc(fpr, tpr)</span>
<span id="cb12-295"><a href="#cb12-295" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-296"><a href="#cb12-296" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the ROC curve</span></span>
<span id="cb12-297"><a href="#cb12-297" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">6</span>))</span>
<span id="cb12-298"><a href="#cb12-298" aria-hidden="true" tabindex="-1"></a>plt.plot(fpr, tpr, color<span class="op">=</span><span class="st">'darkorange'</span>, lw<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="ss">f'ROC curve (AUC = </span><span class="sc">{</span>roc_auc<span class="sc">:.2f}</span><span class="ss">)'</span>)</span>
<span id="cb12-299"><a href="#cb12-299" aria-hidden="true" tabindex="-1"></a>plt.plot([<span class="dv">0</span>, <span class="dv">1</span>], [<span class="dv">0</span>, <span class="dv">1</span>], color<span class="op">=</span><span class="st">'navy'</span>, lw<span class="op">=</span><span class="dv">2</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, label<span class="op">=</span><span class="st">'Random'</span>)</span>
<span id="cb12-300"><a href="#cb12-300" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'False Positive Rate'</span>)</span>
<span id="cb12-301"><a href="#cb12-301" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'True Positive Rate'</span>)</span>
<span id="cb12-302"><a href="#cb12-302" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Receiver Operating Characteristic (ROC) Curve'</span>)</span>
<span id="cb12-303"><a href="#cb12-303" aria-hidden="true" tabindex="-1"></a>plt.legend(loc<span class="op">=</span><span class="st">'lower right'</span>)</span>
<span id="cb12-304"><a href="#cb12-304" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb12-305"><a href="#cb12-305" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-306"><a href="#cb12-306" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb12-307"><a href="#cb12-307" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-308"><a href="#cb12-308" aria-hidden="true" tabindex="-1"></a>This code uses the Breast Cancer dataset, and you can replace it with any binary classification dataset of your choice. The ROC curve is a useful tool for evaluating the performance of binary classification models.</span>
<span id="cb12-309"><a href="#cb12-309" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-310"><a href="#cb12-310" aria-hidden="true" tabindex="-1"></a><span class="fu">### **Precision-Recall Curve:**</span></span>
<span id="cb12-311"><a href="#cb12-311" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-312"><a href="#cb12-312" aria-hidden="true" tabindex="-1"></a>Precision-Recall curves help evaluate models under different class distribution scenarios.</span>
<span id="cb12-313"><a href="#cb12-313" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-316"><a href="#cb12-316" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb12-317"><a href="#cb12-317" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb12-318"><a href="#cb12-318" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb12-319"><a href="#cb12-319" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> load_breast_cancer</span>
<span id="cb12-320"><a href="#cb12-320" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb12-321"><a href="#cb12-321" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.svm <span class="im">import</span> SVC</span>
<span id="cb12-322"><a href="#cb12-322" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> precision_recall_curve, auc, average_precision_score</span>
<span id="cb12-323"><a href="#cb12-323" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-324"><a href="#cb12-324" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the Breast Cancer dataset</span></span>
<span id="cb12-325"><a href="#cb12-325" aria-hidden="true" tabindex="-1"></a>cancer <span class="op">=</span> load_breast_cancer()</span>
<span id="cb12-326"><a href="#cb12-326" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> cancer.data</span>
<span id="cb12-327"><a href="#cb12-327" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> cancer.target</span>
<span id="cb12-328"><a href="#cb12-328" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-329"><a href="#cb12-329" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert the problem to binary classification (class 0 vs class 1)</span></span>
<span id="cb12-330"><a href="#cb12-330" aria-hidden="true" tabindex="-1"></a>y_binary <span class="op">=</span> (y <span class="op">==</span> <span class="dv">0</span>).astype(<span class="bu">int</span>)</span>
<span id="cb12-331"><a href="#cb12-331" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-332"><a href="#cb12-332" aria-hidden="true" tabindex="-1"></a><span class="co"># Split the data into training and testing sets</span></span>
<span id="cb12-333"><a href="#cb12-333" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y_binary, test_size<span class="op">=</span><span class="fl">0.25</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb12-334"><a href="#cb12-334" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-335"><a href="#cb12-335" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a simple SVM model for binary classification</span></span>
<span id="cb12-336"><a href="#cb12-336" aria-hidden="true" tabindex="-1"></a>svm_model <span class="op">=</span> SVC(kernel<span class="op">=</span><span class="st">'linear'</span>, probability<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb12-337"><a href="#cb12-337" aria-hidden="true" tabindex="-1"></a>svm_model.fit(X_train, y_train)</span>
<span id="cb12-338"><a href="#cb12-338" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-339"><a href="#cb12-339" aria-hidden="true" tabindex="-1"></a><span class="co"># Obtain class probabilities for positive class (class 0)</span></span>
<span id="cb12-340"><a href="#cb12-340" aria-hidden="true" tabindex="-1"></a>y_probabilities <span class="op">=</span> svm_model.predict_proba(X_test)[:, <span class="dv">1</span>]</span>
<span id="cb12-341"><a href="#cb12-341" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-342"><a href="#cb12-342" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute Precision-Recall curve and AUC</span></span>
<span id="cb12-343"><a href="#cb12-343" aria-hidden="true" tabindex="-1"></a>precision, recall, _ <span class="op">=</span> precision_recall_curve(y_test, y_probabilities)</span>
<span id="cb12-344"><a href="#cb12-344" aria-hidden="true" tabindex="-1"></a>pr_auc <span class="op">=</span> auc(recall, precision)</span>
<span id="cb12-345"><a href="#cb12-345" aria-hidden="true" tabindex="-1"></a>average_precision <span class="op">=</span> average_precision_score(y_test, y_probabilities)</span>
<span id="cb12-346"><a href="#cb12-346" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-347"><a href="#cb12-347" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the Precision-Recall curve</span></span>
<span id="cb12-348"><a href="#cb12-348" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">6</span>))</span>
<span id="cb12-349"><a href="#cb12-349" aria-hidden="true" tabindex="-1"></a>plt.plot(recall, precision, color<span class="op">=</span><span class="st">'darkorange'</span>, lw<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="ss">f'PR curve (AUC = </span><span class="sc">{</span>pr_auc<span class="sc">:.2f}</span><span class="ss">, Avg. Precision = </span><span class="sc">{</span>average_precision<span class="sc">:.2f}</span><span class="ss">)'</span>)</span>
<span id="cb12-350"><a href="#cb12-350" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Recall'</span>)</span>
<span id="cb12-351"><a href="#cb12-351" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Precision'</span>)</span>
<span id="cb12-352"><a href="#cb12-352" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Precision-Recall Curve'</span>)</span>
<span id="cb12-353"><a href="#cb12-353" aria-hidden="true" tabindex="-1"></a>plt.legend(loc<span class="op">=</span><span class="st">'lower right'</span>)</span>
<span id="cb12-354"><a href="#cb12-354" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb12-355"><a href="#cb12-355" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-356"><a href="#cb12-356" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-357"><a href="#cb12-357" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb12-358"><a href="#cb12-358" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-359"><a href="#cb12-359" aria-hidden="true" tabindex="-1"></a>This code demonstrates how to create a Precision-Recall curve for a binary classification problem using an SVM model. Replace the dataset with any binary classification dataset of your choice. The Precision-Recall curve provides valuable insights into the performance of a classifier, especially in imbalanced datasets.</span>
<span id="cb12-360"><a href="#cb12-360" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-361"><a href="#cb12-361" aria-hidden="true" tabindex="-1"></a><span class="fu">### **Mathematical Explanation of Classification in Machine Learning:**</span></span>
<span id="cb12-362"><a href="#cb12-362" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-363"><a href="#cb12-363" aria-hidden="true" tabindex="-1"></a>Classification in machine learning is a type of supervised learning where the algorithm learns to categorize input data into predefined classes or labels. The goal is to train the model to make accurate predictions on new, unseen data based on patterns learned during training.</span>
<span id="cb12-364"><a href="#cb12-364" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-365"><a href="#cb12-365" aria-hidden="true" tabindex="-1"></a>**Binary Classification:** In binary classification, there are two possible outcomes or classes, often denoted as positive (1) and negative (0). The algorithm learns a decision boundary to separate these classes in the feature space.</span>
<span id="cb12-366"><a href="#cb12-366" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-367"><a href="#cb12-367" aria-hidden="true" tabindex="-1"></a>Here are the mathematical formulas for key binary classification metrics:</span>
<span id="cb12-368"><a href="#cb12-368" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-369"><a href="#cb12-369" aria-hidden="true" tabindex="-1"></a>**True Positive (TP):**</span>
<span id="cb12-370"><a href="#cb12-370" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-371"><a href="#cb12-371" aria-hidden="true" tabindex="-1"></a>TP=Number&nbsp;of&nbsp;instances&nbsp;correctly&nbsp;predicted&nbsp;as&nbsp;positive</span>
<span id="cb12-372"><a href="#cb12-372" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-373"><a href="#cb12-373" aria-hidden="true" tabindex="-1"></a>**True Negative (TN):**</span>
<span id="cb12-374"><a href="#cb12-374" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-375"><a href="#cb12-375" aria-hidden="true" tabindex="-1"></a>TN=Number&nbsp;of&nbsp;instances&nbsp;correctly&nbsp;predicted&nbsp;as&nbsp;negative</span>
<span id="cb12-376"><a href="#cb12-376" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-377"><a href="#cb12-377" aria-hidden="true" tabindex="-1"></a>**False Positive (FP):**</span>
<span id="cb12-378"><a href="#cb12-378" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-379"><a href="#cb12-379" aria-hidden="true" tabindex="-1"></a>FP=Number&nbsp;of&nbsp;instances&nbsp;incorrectly&nbsp;predicted&nbsp;as&nbsp;positive&nbsp;(Type&nbsp;I&nbsp;error)</span>
<span id="cb12-380"><a href="#cb12-380" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-381"><a href="#cb12-381" aria-hidden="true" tabindex="-1"></a>**False Negative (FN):**</span>
<span id="cb12-382"><a href="#cb12-382" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-383"><a href="#cb12-383" aria-hidden="true" tabindex="-1"></a>FN=Number&nbsp;of&nbsp;instances&nbsp;incorrectly&nbsp;predicted&nbsp;as&nbsp;negative&nbsp;(Type&nbsp;II&nbsp;error)</span>
<span id="cb12-384"><a href="#cb12-384" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-385"><a href="#cb12-385" aria-hidden="true" tabindex="-1"></a>**Accuracy:**</span>
<span id="cb12-386"><a href="#cb12-386" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-387"><a href="#cb12-387" aria-hidden="true" tabindex="-1"></a>Accuracy=TP+TN/(FP+FN+TP+TN)​</span>
<span id="cb12-388"><a href="#cb12-388" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-389"><a href="#cb12-389" aria-hidden="true" tabindex="-1"></a>**Precision (Positive Predictive Value):**</span>
<span id="cb12-390"><a href="#cb12-390" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-391"><a href="#cb12-391" aria-hidden="true" tabindex="-1"></a>Precision=TP/(FP+TP)​</span>
<span id="cb12-392"><a href="#cb12-392" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-393"><a href="#cb12-393" aria-hidden="true" tabindex="-1"></a>**Recall (Sensitivity, True Positive Rate):**</span>
<span id="cb12-394"><a href="#cb12-394" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-395"><a href="#cb12-395" aria-hidden="true" tabindex="-1"></a>Recall=TP/(FN+TP)​</span>
<span id="cb12-396"><a href="#cb12-396" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-397"><a href="#cb12-397" aria-hidden="true" tabindex="-1"></a>**Specificity (True Negative Rate):**</span>
<span id="cb12-398"><a href="#cb12-398" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-399"><a href="#cb12-399" aria-hidden="true" tabindex="-1"></a>Specificity=TN/(FP+TN)​</span>
<span id="cb12-400"><a href="#cb12-400" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-401"><a href="#cb12-401" aria-hidden="true" tabindex="-1"></a>**F1 Score:**</span>
<span id="cb12-402"><a href="#cb12-402" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-403"><a href="#cb12-403" aria-hidden="true" tabindex="-1"></a>F1Score=(2⋅Precision⋅Recall​)/(Precision+Recall)</span>
<span id="cb12-404"><a href="#cb12-404" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-405"><a href="#cb12-405" aria-hidden="true" tabindex="-1"></a>**False Positive Rate (FPR):**</span>
<span id="cb12-406"><a href="#cb12-406" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-407"><a href="#cb12-407" aria-hidden="true" tabindex="-1"></a>FPR=FP/(TN+FP)​</span>
<span id="cb12-408"><a href="#cb12-408" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-409"><a href="#cb12-409" aria-hidden="true" tabindex="-1"></a>**False Negative Rate (FNR):**</span>
<span id="cb12-410"><a href="#cb12-410" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-411"><a href="#cb12-411" aria-hidden="true" tabindex="-1"></a>FNR=FN/(TP+FN)​</span>
<span id="cb12-412"><a href="#cb12-412" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-413"><a href="#cb12-413" aria-hidden="true" tabindex="-1"></a>**Area Under the ROC Curve (AUC-ROC):**</span>
<span id="cb12-414"><a href="#cb12-414" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-415"><a href="#cb12-415" aria-hidden="true" tabindex="-1"></a>AUC-ROC(Area&nbsp;Under&nbsp;the&nbsp;Receiver&nbsp;Operating&nbsp;Characteristic&nbsp;Curve)</span>
<span id="cb12-416"><a href="#cb12-416" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-417"><a href="#cb12-417" aria-hidden="true" tabindex="-1"></a>These formulas provide a quantitative assessment of a binary classification model's performance by considering different aspects such as accuracy, precision, recall, specificity, and the trade-off between precision and recall captured by the F1 score. The AUC-ROC measures the area under the curve that illustrates the model's true positive rate against the false positive rate at various decision thresholds.</span>
<span id="cb12-418"><a href="#cb12-418" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-419"><a href="#cb12-419" aria-hidden="true" tabindex="-1"></a>**Multiclass Classification:** For multiclass classification, there are more than two classes, and the model learns to distinguish between all possible classes. It involves assigning an input to one of several predefined categories.</span>
<span id="cb12-420"><a href="#cb12-420" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-421"><a href="#cb12-421" aria-hidden="true" tabindex="-1"></a>**Logistic Regression:** Logistic Regression is a commonly used algorithm for binary classification. It models the probability of an instance belonging to a particular class using the logistic function. The decision boundary is determined by a set of weights and biases.</span>
<span id="cb12-422"><a href="#cb12-422" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-423"><a href="#cb12-423" aria-hidden="true" tabindex="-1"></a>**Support Vector Machines (SVM):** SVM is another algorithm that finds the optimal hyperplane to separate classes. It works well for both binary and multiclass classification, aiming to maximize the margin between different classes.</span>
<span id="cb12-424"><a href="#cb12-424" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-427"><a href="#cb12-427" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb12-428"><a href="#cb12-428" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb12-429"><a href="#cb12-429" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb12-430"><a href="#cb12-430" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb12-431"><a href="#cb12-431" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</span>
<span id="cb12-432"><a href="#cb12-432" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> roc_curve, auc, precision_recall_curve, confusion_matrix</span>
<span id="cb12-433"><a href="#cb12-433" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-434"><a href="#cb12-434" aria-hidden="true" tabindex="-1"></a><span class="co"># Create example data</span></span>
<span id="cb12-435"><a href="#cb12-435" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.random.rand(<span class="dv">100</span>, <span class="dv">2</span>)</span>
<span id="cb12-436"><a href="#cb12-436" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> (X[:, <span class="dv">0</span>] <span class="op">+</span> X[:, <span class="dv">1</span>] <span class="op">&gt;</span> <span class="dv">1</span>).astype(<span class="bu">int</span>)</span>
<span id="cb12-437"><a href="#cb12-437" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-438"><a href="#cb12-438" aria-hidden="true" tabindex="-1"></a><span class="co"># Split the data into training and testing sets</span></span>
<span id="cb12-439"><a href="#cb12-439" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb12-440"><a href="#cb12-440" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-441"><a href="#cb12-441" aria-hidden="true" tabindex="-1"></a><span class="co"># Train a logistic regression model</span></span>
<span id="cb12-442"><a href="#cb12-442" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> LogisticRegression()</span>
<span id="cb12-443"><a href="#cb12-443" aria-hidden="true" tabindex="-1"></a>model.fit(X_train, y_train)</span>
<span id="cb12-444"><a href="#cb12-444" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-445"><a href="#cb12-445" aria-hidden="true" tabindex="-1"></a><span class="co"># Predict probabilities and classes</span></span>
<span id="cb12-446"><a href="#cb12-446" aria-hidden="true" tabindex="-1"></a>y_probs <span class="op">=</span> model.predict_proba(X_test)[:, <span class="dv">1</span>]</span>
<span id="cb12-447"><a href="#cb12-447" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> model.predict(X_test)</span>
<span id="cb12-448"><a href="#cb12-448" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-449"><a href="#cb12-449" aria-hidden="true" tabindex="-1"></a><span class="co"># ROC Curve and AUC</span></span>
<span id="cb12-450"><a href="#cb12-450" aria-hidden="true" tabindex="-1"></a>fpr, tpr, _ <span class="op">=</span> roc_curve(y_test, y_probs)</span>
<span id="cb12-451"><a href="#cb12-451" aria-hidden="true" tabindex="-1"></a>roc_auc <span class="op">=</span> auc(fpr, tpr)</span>
<span id="cb12-452"><a href="#cb12-452" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-453"><a href="#cb12-453" aria-hidden="true" tabindex="-1"></a><span class="co"># Precision-Recall Curve</span></span>
<span id="cb12-454"><a href="#cb12-454" aria-hidden="true" tabindex="-1"></a>precision, recall, _ <span class="op">=</span> precision_recall_curve(y_test, y_probs)</span>
<span id="cb12-455"><a href="#cb12-455" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-456"><a href="#cb12-456" aria-hidden="true" tabindex="-1"></a><span class="co"># Confusion Matrix</span></span>
<span id="cb12-457"><a href="#cb12-457" aria-hidden="true" tabindex="-1"></a>conf_matrix <span class="op">=</span> confusion_matrix(y_test, y_pred)</span>
<span id="cb12-458"><a href="#cb12-458" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-459"><a href="#cb12-459" aria-hidden="true" tabindex="-1"></a><span class="co"># Print or visualize metrics as needed</span></span>
<span id="cb12-460"><a href="#cb12-460" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Area under ROC Curve (AUC): </span><span class="sc">{</span>roc_auc<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb12-461"><a href="#cb12-461" aria-hidden="true" tabindex="-1"></a>plt.plot(fpr, tpr, color<span class="op">=</span><span class="st">'darkorange'</span>, lw<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'ROC Curve'</span>)</span>
<span id="cb12-462"><a href="#cb12-462" aria-hidden="true" tabindex="-1"></a>plt.plot([<span class="dv">0</span>, <span class="dv">1</span>], [<span class="dv">0</span>, <span class="dv">1</span>], color<span class="op">=</span><span class="st">'navy'</span>, lw<span class="op">=</span><span class="dv">2</span>, linestyle<span class="op">=</span><span class="st">'--'</span>)</span>
<span id="cb12-463"><a href="#cb12-463" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'False Positive Rate'</span>)</span>
<span id="cb12-464"><a href="#cb12-464" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'True Positive Rate'</span>)</span>
<span id="cb12-465"><a href="#cb12-465" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Receiver Operating Characteristic (ROC) Curve'</span>)</span>
<span id="cb12-466"><a href="#cb12-466" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb12-467"><a href="#cb12-467" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb12-468"><a href="#cb12-468" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb12-469"><a href="#cb12-469" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-470"><a href="#cb12-470" aria-hidden="true" tabindex="-1"></a>This code demonstrates a basic binary classification scenario with logistic regression and includes the calculation and visualization of ROC Curve, AUC, Precision-Recall Curve, and a Confusion Matrix. Adjustments can be made for multiclass classification scenarios.</span>
<span id="cb12-471"><a href="#cb12-471" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-474"><a href="#cb12-474" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb12-475"><a href="#cb12-475" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb12-476"><a href="#cb12-476" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb12-477"><a href="#cb12-477" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb12-478"><a href="#cb12-478" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</span>
<span id="cb12-479"><a href="#cb12-479" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> precision_recall_curve, auc</span>
<span id="cb12-480"><a href="#cb12-480" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-481"><a href="#cb12-481" aria-hidden="true" tabindex="-1"></a><span class="co"># Create example data</span></span>
<span id="cb12-482"><a href="#cb12-482" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.random.rand(<span class="dv">100</span>, <span class="dv">2</span>)</span>
<span id="cb12-483"><a href="#cb12-483" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> (X[:, <span class="dv">0</span>] <span class="op">+</span> X[:, <span class="dv">1</span>] <span class="op">&gt;</span> <span class="dv">1</span>).astype(<span class="bu">int</span>)</span>
<span id="cb12-484"><a href="#cb12-484" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-485"><a href="#cb12-485" aria-hidden="true" tabindex="-1"></a><span class="co"># Split the data into training and testing sets</span></span>
<span id="cb12-486"><a href="#cb12-486" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb12-487"><a href="#cb12-487" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-488"><a href="#cb12-488" aria-hidden="true" tabindex="-1"></a><span class="co"># Train a logistic regression model</span></span>
<span id="cb12-489"><a href="#cb12-489" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> LogisticRegression()</span>
<span id="cb12-490"><a href="#cb12-490" aria-hidden="true" tabindex="-1"></a>model.fit(X_train, y_train)</span>
<span id="cb12-491"><a href="#cb12-491" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-492"><a href="#cb12-492" aria-hidden="true" tabindex="-1"></a><span class="co"># Predict probabilities</span></span>
<span id="cb12-493"><a href="#cb12-493" aria-hidden="true" tabindex="-1"></a>y_probs <span class="op">=</span> model.predict_proba(X_test)[:, <span class="dv">1</span>]</span>
<span id="cb12-494"><a href="#cb12-494" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-495"><a href="#cb12-495" aria-hidden="true" tabindex="-1"></a><span class="co"># Precision-Recall Curve</span></span>
<span id="cb12-496"><a href="#cb12-496" aria-hidden="true" tabindex="-1"></a>precision, recall, _ <span class="op">=</span> precision_recall_curve(y_test, y_probs)</span>
<span id="cb12-497"><a href="#cb12-497" aria-hidden="true" tabindex="-1"></a>pr_auc <span class="op">=</span> auc(recall, precision)</span>
<span id="cb12-498"><a href="#cb12-498" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-499"><a href="#cb12-499" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot Precision-Recall Curve</span></span>
<span id="cb12-500"><a href="#cb12-500" aria-hidden="true" tabindex="-1"></a>plt.plot(recall, precision, color<span class="op">=</span><span class="st">'blue'</span>, lw<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="ss">f'PR Curve (AUC = </span><span class="sc">{</span>pr_auc<span class="sc">:.2f}</span><span class="ss">)'</span>)</span>
<span id="cb12-501"><a href="#cb12-501" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Recall'</span>)</span>
<span id="cb12-502"><a href="#cb12-502" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Precision'</span>)</span>
<span id="cb12-503"><a href="#cb12-503" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Precision-Recall Curve'</span>)</span>
<span id="cb12-504"><a href="#cb12-504" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb12-505"><a href="#cb12-505" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb12-506"><a href="#cb12-506" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb12-507"><a href="#cb12-507" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-508"><a href="#cb12-508" aria-hidden="true" tabindex="-1"></a>The provided Python code generates a Precision-Recall (PR) Curve for a binary classification scenario. It uses a logistic regression model trained on a synthetic dataset. The dataset is split into training and testing sets, and the logistic regression model is fitted on the training data. The model's predicted probabilities for the positive class on the test set are then used to plot the PR Curve. The curve illustrates the trade-off between precision and recall at various decision thresholds. The area under the PR Curve (AUC) is calculated and displayed in the plot, providing a quantitative measure of model performance.</span>
<span id="cb12-509"><a href="#cb12-509" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-512"><a href="#cb12-512" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb12-513"><a href="#cb12-513" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb12-514"><a href="#cb12-514" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb12-515"><a href="#cb12-515" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb12-516"><a href="#cb12-516" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</span>
<span id="cb12-517"><a href="#cb12-517" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> confusion_matrix</span>
<span id="cb12-518"><a href="#cb12-518" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb12-519"><a href="#cb12-519" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-520"><a href="#cb12-520" aria-hidden="true" tabindex="-1"></a><span class="co"># Create example data</span></span>
<span id="cb12-521"><a href="#cb12-521" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.random.rand(<span class="dv">100</span>, <span class="dv">2</span>)</span>
<span id="cb12-522"><a href="#cb12-522" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> (X[:, <span class="dv">0</span>] <span class="op">+</span> X[:, <span class="dv">1</span>] <span class="op">&gt;</span> <span class="dv">1</span>).astype(<span class="bu">int</span>)</span>
<span id="cb12-523"><a href="#cb12-523" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-524"><a href="#cb12-524" aria-hidden="true" tabindex="-1"></a><span class="co"># Split the data into training and testing sets</span></span>
<span id="cb12-525"><a href="#cb12-525" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb12-526"><a href="#cb12-526" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-527"><a href="#cb12-527" aria-hidden="true" tabindex="-1"></a><span class="co"># Train a logistic regression model</span></span>
<span id="cb12-528"><a href="#cb12-528" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> LogisticRegression()</span>
<span id="cb12-529"><a href="#cb12-529" aria-hidden="true" tabindex="-1"></a>model.fit(X_train, y_train)</span>
<span id="cb12-530"><a href="#cb12-530" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-531"><a href="#cb12-531" aria-hidden="true" tabindex="-1"></a><span class="co"># Predict classes</span></span>
<span id="cb12-532"><a href="#cb12-532" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> model.predict(X_test)</span>
<span id="cb12-533"><a href="#cb12-533" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-534"><a href="#cb12-534" aria-hidden="true" tabindex="-1"></a><span class="co"># Confusion Matrix</span></span>
<span id="cb12-535"><a href="#cb12-535" aria-hidden="true" tabindex="-1"></a>conf_matrix <span class="op">=</span> confusion_matrix(y_test, y_pred)</span>
<span id="cb12-536"><a href="#cb12-536" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-537"><a href="#cb12-537" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot Confusion Matrix</span></span>
<span id="cb12-538"><a href="#cb12-538" aria-hidden="true" tabindex="-1"></a>sns.heatmap(conf_matrix, annot<span class="op">=</span><span class="va">True</span>, fmt<span class="op">=</span><span class="st">'d'</span>, cmap<span class="op">=</span><span class="st">'Blues'</span>, cbar<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb12-539"><a href="#cb12-539" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Predicted Labels'</span>)</span>
<span id="cb12-540"><a href="#cb12-540" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'True Labels'</span>)</span>
<span id="cb12-541"><a href="#cb12-541" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Confusion Matrix'</span>)</span>
<span id="cb12-542"><a href="#cb12-542" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb12-543"><a href="#cb12-543" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-544"><a href="#cb12-544" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb12-545"><a href="#cb12-545" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-546"><a href="#cb12-546" aria-hidden="true" tabindex="-1"></a>The presented Python code produces a Confusion Matrix for a binary classification task using a logistic regression model. Similar to the PR Curve code, it utilizes a synthetic dataset split into training and testing sets. The logistic regression model is trained on the training data, and its predictions on the test set are used to construct the confusion matrix. The matrix visualizes the model's performance by showing the counts of true positive, true negative, false positive, and false negative predictions. The Seaborn library is employed to create a heatmap for clearer visualization of the confusion matrix. This allows for a detailed assessment of the model's ability to correctly classify instances into positive and negative classes.</span>
<span id="cb12-547"><a href="#cb12-547" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-550"><a href="#cb12-550" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb12-551"><a href="#cb12-551" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb12-552"><a href="#cb12-552" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb12-553"><a href="#cb12-553" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb12-554"><a href="#cb12-554" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> confusion_matrix, roc_curve, auc, precision_recall_curve, average_precision_score</span>
<span id="cb12-555"><a href="#cb12-555" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score, precision_score, recall_score, f1_score</span>
<span id="cb12-556"><a href="#cb12-556" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-557"><a href="#cb12-557" aria-hidden="true" tabindex="-1"></a><span class="co"># Sample true labels and predicted labels</span></span>
<span id="cb12-558"><a href="#cb12-558" aria-hidden="true" tabindex="-1"></a>y_true <span class="op">=</span> np.array([<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>])</span>
<span id="cb12-559"><a href="#cb12-559" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> np.array([<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">0</span>])</span>
<span id="cb12-560"><a href="#cb12-560" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-561"><a href="#cb12-561" aria-hidden="true" tabindex="-1"></a><span class="co"># Get different types of confusion matrices</span></span>
<span id="cb12-562"><a href="#cb12-562" aria-hidden="true" tabindex="-1"></a>conf_matrix <span class="op">=</span> confusion_matrix(y_true, y_pred)</span>
<span id="cb12-563"><a href="#cb12-563" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-564"><a href="#cb12-564" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate additional metrics</span></span>
<span id="cb12-565"><a href="#cb12-565" aria-hidden="true" tabindex="-1"></a>accuracy <span class="op">=</span> accuracy_score(y_true, y_pred)</span>
<span id="cb12-566"><a href="#cb12-566" aria-hidden="true" tabindex="-1"></a>precision <span class="op">=</span> precision_score(y_true, y_pred)</span>
<span id="cb12-567"><a href="#cb12-567" aria-hidden="true" tabindex="-1"></a>recall <span class="op">=</span> recall_score(y_true, y_pred)</span>
<span id="cb12-568"><a href="#cb12-568" aria-hidden="true" tabindex="-1"></a>f1 <span class="op">=</span> f1_score(y_true, y_pred)</span>
<span id="cb12-569"><a href="#cb12-569" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-570"><a href="#cb12-570" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate ROC curve</span></span>
<span id="cb12-571"><a href="#cb12-571" aria-hidden="true" tabindex="-1"></a>fpr, tpr, _ <span class="op">=</span> roc_curve(y_true, y_pred)</span>
<span id="cb12-572"><a href="#cb12-572" aria-hidden="true" tabindex="-1"></a>roc_auc <span class="op">=</span> auc(fpr, tpr)</span>
<span id="cb12-573"><a href="#cb12-573" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-574"><a href="#cb12-574" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate Precision-Recall curve</span></span>
<span id="cb12-575"><a href="#cb12-575" aria-hidden="true" tabindex="-1"></a>precision_curve, recall_curve, _ <span class="op">=</span> precision_recall_curve(y_true, y_pred)</span>
<span id="cb12-576"><a href="#cb12-576" aria-hidden="true" tabindex="-1"></a>pr_auc <span class="op">=</span> average_precision_score(y_true, y_pred)</span>
<span id="cb12-577"><a href="#cb12-577" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-578"><a href="#cb12-578" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a figure with subplots</span></span>
<span id="cb12-579"><a href="#cb12-579" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">4</span>, figsize<span class="op">=</span>(<span class="dv">15</span>, <span class="dv">4</span>))</span>
<span id="cb12-580"><a href="#cb12-580" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-581"><a href="#cb12-581" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot confusion matrices</span></span>
<span id="cb12-582"><a href="#cb12-582" aria-hidden="true" tabindex="-1"></a>sns.heatmap(conf_matrix, annot<span class="op">=</span><span class="va">True</span>, fmt<span class="op">=</span><span class="st">"d"</span>, cmap<span class="op">=</span><span class="st">"Blues"</span>, cbar<span class="op">=</span><span class="va">False</span>, ax<span class="op">=</span>axes[<span class="dv">0</span>])</span>
<span id="cb12-583"><a href="#cb12-583" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_title(<span class="st">'Confusion Matrix'</span>)</span>
<span id="cb12-584"><a href="#cb12-584" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-585"><a href="#cb12-585" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot additional metrics</span></span>
<span id="cb12-586"><a href="#cb12-586" aria-hidden="true" tabindex="-1"></a>sns.barplot(x<span class="op">=</span>[<span class="st">'Accuracy'</span>, <span class="st">'Precision'</span>, <span class="st">'Recall'</span>, <span class="st">'F1 Score'</span>], y<span class="op">=</span>[accuracy, precision, recall, f1], ax<span class="op">=</span>axes[<span class="dv">1</span>])</span>
<span id="cb12-587"><a href="#cb12-587" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_title(<span class="st">'Additional Metrics'</span>)</span>
<span id="cb12-588"><a href="#cb12-588" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-589"><a href="#cb12-589" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot ROC Curve</span></span>
<span id="cb12-590"><a href="#cb12-590" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">2</span>].plot(fpr, tpr, color<span class="op">=</span><span class="st">'darkorange'</span>, lw<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'ROC curve (area = </span><span class="sc">{:.2f}</span><span class="st">)'</span>.<span class="bu">format</span>(roc_auc))</span>
<span id="cb12-591"><a href="#cb12-591" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">2</span>].plot([<span class="dv">0</span>, <span class="dv">1</span>], [<span class="dv">0</span>, <span class="dv">1</span>], color<span class="op">=</span><span class="st">'navy'</span>, lw<span class="op">=</span><span class="dv">2</span>, linestyle<span class="op">=</span><span class="st">'--'</span>)</span>
<span id="cb12-592"><a href="#cb12-592" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">2</span>].set_xlim([<span class="fl">0.0</span>, <span class="fl">1.0</span>])</span>
<span id="cb12-593"><a href="#cb12-593" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">2</span>].set_ylim([<span class="fl">0.0</span>, <span class="fl">1.05</span>])</span>
<span id="cb12-594"><a href="#cb12-594" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">2</span>].set_xlabel(<span class="st">'False Positive Rate'</span>)</span>
<span id="cb12-595"><a href="#cb12-595" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">2</span>].set_ylabel(<span class="st">'True Positive Rate'</span>)</span>
<span id="cb12-596"><a href="#cb12-596" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">2</span>].set_title(<span class="st">'ROC Curve'</span>)</span>
<span id="cb12-597"><a href="#cb12-597" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">2</span>].legend(loc<span class="op">=</span><span class="st">"lower right"</span>)</span>
<span id="cb12-598"><a href="#cb12-598" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-599"><a href="#cb12-599" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot Precision-Recall Curve</span></span>
<span id="cb12-600"><a href="#cb12-600" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">3</span>].plot(recall_curve, precision_curve, color<span class="op">=</span><span class="st">'darkorange'</span>, lw<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'Precision-Recall curve (area = </span><span class="sc">{:.2f}</span><span class="st">)'</span>.<span class="bu">format</span>(pr_auc))</span>
<span id="cb12-601"><a href="#cb12-601" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">3</span>].set_xlim([<span class="fl">0.0</span>, <span class="fl">1.0</span>])</span>
<span id="cb12-602"><a href="#cb12-602" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">3</span>].set_ylim([<span class="fl">0.0</span>, <span class="fl">1.05</span>])</span>
<span id="cb12-603"><a href="#cb12-603" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">3</span>].set_xlabel(<span class="st">'Recall'</span>)</span>
<span id="cb12-604"><a href="#cb12-604" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">3</span>].set_ylabel(<span class="st">'Precision'</span>)</span>
<span id="cb12-605"><a href="#cb12-605" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">3</span>].set_title(<span class="st">'Precision-Recall Curve'</span>)</span>
<span id="cb12-606"><a href="#cb12-606" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">3</span>].legend(loc<span class="op">=</span><span class="st">"lower right"</span>)</span>
<span id="cb12-607"><a href="#cb12-607" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-608"><a href="#cb12-608" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb12-609"><a href="#cb12-609" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb12-610"><a href="#cb12-610" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-611"><a href="#cb12-611" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb12-612"><a href="#cb12-612" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-613"><a href="#cb12-613" aria-hidden="true" tabindex="-1"></a>This code uses a sample set of true labels (y_true) and predicted labels (y_pred). It calculates the confusion matrix and additional metrics such as accuracy, precision, recall, and F1 score. The results are visualized using Seaborn in a side-by-side format with subplots for the confusion matrix, additional metrics, ROC Curve, and Precision-Recall Curve. Make sure to adjust the labels and data according to your specific use case.</span>
<span id="cb12-614"><a href="#cb12-614" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-615"><a href="#cb12-615" aria-hidden="true" tabindex="-1"></a><span class="fu">## **Conclusion:**</span></span>
<span id="cb12-616"><a href="#cb12-616" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-617"><a href="#cb12-617" aria-hidden="true" tabindex="-1"></a>Classification algorithms are indispensable tools for extracting patterns and making predictions across a spectrum of applications. The Python examples, complemented by advanced plots and evaluation metrics, provide a glimpse into the richness and versatility of classification in machine learning. As we navigate the landscape of classification, these techniques empower us to unravel complex relationships and contribute meaningfully to data-driven decision-making.</span>
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">M Mubashar</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/Mubashar97">
      <i class="bi bi-github" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/mmubashar/">
      <i class="bi bi-linkedin" role="img">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>



</body></html>